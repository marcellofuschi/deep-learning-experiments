{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENdpIzLyycFw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Character-level LSTM with PyTorch\n",
    "\n",
    "An RNN model will be trained to generate new text character by character."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install textstat"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iY7R0wQFBgpN",
    "outputId": "fcc9ccab-e4a9-4e5b-cbdf-0e919ac51e9c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: textstat in /usr/local/lib/python3.8/dist-packages (0.7.3)\n",
      "Requirement already satisfied: pyphen in /usr/local/lib/python3.8/dist-packages (from textstat) (0.13.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "tsoK1d67ycF1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JewJr-vMycF2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "DP6f7VrkycF2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('html_physics_book.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-aoQoHIycF3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Characters tokenization\n",
    "\n",
    "We need to be able to convert each character into an integer token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "_ylyruewycF3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chars_in_text = tuple(set(text))\n",
    "\n",
    "int2char = dict(enumerate(chars_in_text))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded_text = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Encoding and then decoding produces the initial character\n",
    "print(int2char[char2int['X']])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CsTcQHbOmIXW",
    "outputId": "8b4851bc-8b12-4e38-aeaa-e7b086e6c4b4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naZeJhL6ycF6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The LSTM accepts only one-hot encoded vectors, so we prepare a function that takes as input an array of encoded characters and outputs an array of one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Lyerrn0jycF7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "341a23ad-ac53-4796-ef69-90b01ec0ab46",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "def to_one_hot(char_encoding):\n",
    "    one_hot = np.zeros(len(int2char))\n",
    "    one_hot[char_encoding] = 1\n",
    "    return one_hot\n",
    "\n",
    "# Test\n",
    "to_one_hot(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0fYgSq2ycF7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Making training mini-batches\n",
    "\n",
    "We want to create both the input and target arrays, with the targets being the same as the inputs, but shifted over one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "SPrN0R-aycF8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Divide the `encodings_arr` array into batches.\n",
    "def get_batches(encodings_arr, sequences_per_batch, chars_per_sequence):\n",
    "    batch_size = sequences_per_batch * chars_per_sequence\n",
    "    number_of_batches = len(encodings_arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    encodings_arr = encodings_arr[:number_of_batches * batch_size]\n",
    "    \n",
    "    # Split the array into sequences by reshaping it into `sequences_per_batch` rows.\n",
    "    encodings_arr = encodings_arr.reshape((sequences_per_batch, -1))\n",
    "    \n",
    "    for n in range(0, encodings_arr.shape[1], chars_per_sequence):        \n",
    "        # The features\n",
    "        x = encodings_arr[:, n:n+chars_per_sequence]\n",
    "        \n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], encodings_arr[:, n+chars_per_sequence]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], encodings_arr[:, 0]\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test the mini-batches generation"
   ],
   "metadata": {
    "id": "UVfu3TPeuPlU",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "first_batch = next(get_batches(encoded_text, 3, 100))\n",
    "print(f'Every mini-batch is a tuple of {len(first_batch)} arrays.')\n",
    "print(f'First one contains features and in this case has shape {first_batch[0].shape}')\n",
    "print(f'Second one contains targets and in this case has shape {first_batch[1].shape}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5RV7g3posJ77",
    "outputId": "7ffe656e-0410-4c42-faef-8f977ed5db1c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Every mini-batch is a tuple of 2 arrays.\n",
      "First one contains features and in this case has shape (3, 100)\n",
      "Second one contains targets and in this case has shape (3, 100)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "uacfp0MMycF8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6e52e830-cbdc-4bda-b907-a73b0389dcb5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x\n",
      " [[50 67 21 30 31 45 12 86 93  7]\n",
      " [42 72 38  7 41 42 72 38 35  7]\n",
      " [10 72 80  0  5 38  7 75  0  7]]\n",
      "\n",
      "y\n",
      " [[67 21 30 31 45 12 86 93  7 72]\n",
      " [72 38  7 41 42 72 38 35  7 42]\n",
      " [72 80  0  5 38  7 75  0  7 10]]\n"
     ]
    }
   ],
   "source": [
    "x, y = first_batch\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5NJ0QTRycF9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The data is correctly shifted over one step for `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65etXbJ0ycF9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "PoanYVyPycF-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        char_embedding_size = len(int2char) # size of the one-hot vectors representing characters\n",
    "        self.lstm = nn.LSTM(char_embedding_size, n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, char_embedding_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hc):\n",
    "        # Get x, and the new hidden state (h, c) from the lstm\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        x = x.reshape(x.size()[0] * x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return x and the hidden state (h, c)\n",
    "        return x, (h, c)\n",
    "    \n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character and the hidden state, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the new hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[to_one_hot(char2int[char])]], dtype=np.float32)\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([h_item.data for h_item in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(int2char))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        \n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XWWejkPycF_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "bOsuboWKycF_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        h = net.init_hidden(n_seqs)\n",
    "        \n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = np.array([\n",
    "                [to_one_hot(c) for c in seq] for seq in x\n",
    "            ], dtype=np.float32)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            \n",
    "            loss = criterion(\n",
    "                output,\n",
    "                targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor if cuda else torch.LongTensor)\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                \n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    \n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = np.array([\n",
    "                        [to_one_hot(c) for c in seq] for seq in x\n",
    "                    ], dtype=np.float32)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(\n",
    "                        output,\n",
    "                        targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor if cuda else torch.LongTensor)\n",
    "                    )\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T7PX3uPSycGB",
    "outputId": "c9d360a9-8d07-4daf-ab50-1d102cbf40b9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(94, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=94, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = CharRNN(n_hidden=512, n_layers=2)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8NpfNmlHycGB",
    "outputId": "13a8f3e3-e440-413f-c0a8-a7fbbec464b5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/40... Step: 10... Loss: 3.5179... Val Loss: 3.4546\n",
      "Epoch: 1/40... Step: 20... Loss: 3.3317... Val Loss: 3.3077\n",
      "Epoch: 2/40... Step: 30... Loss: 3.1815... Val Loss: 3.0609\n",
      "Epoch: 2/40... Step: 40... Loss: 3.0457... Val Loss: 2.8949\n",
      "Epoch: 3/40... Step: 50... Loss: 2.8641... Val Loss: 2.7264\n",
      "Epoch: 3/40... Step: 60... Loss: 3.2422... Val Loss: 2.7294\n",
      "Epoch: 3/40... Step: 70... Loss: 2.6262... Val Loss: 2.4990\n",
      "Epoch: 4/40... Step: 80... Loss: 2.5227... Val Loss: 2.4204\n",
      "Epoch: 4/40... Step: 90... Loss: 2.4018... Val Loss: 2.3181\n",
      "Epoch: 5/40... Step: 100... Loss: 2.3232... Val Loss: 2.2779\n",
      "Epoch: 5/40... Step: 110... Loss: 2.2593... Val Loss: 2.2520\n",
      "Epoch: 5/40... Step: 120... Loss: 2.1784... Val Loss: 2.1992\n",
      "Epoch: 6/40... Step: 130... Loss: 2.0956... Val Loss: 2.1916\n",
      "Epoch: 6/40... Step: 140... Loss: 2.0826... Val Loss: 2.1828\n",
      "Epoch: 7/40... Step: 150... Loss: 2.0337... Val Loss: 2.1391\n",
      "Epoch: 7/40... Step: 160... Loss: 2.0108... Val Loss: 2.0765\n",
      "Epoch: 8/40... Step: 170... Loss: 1.9419... Val Loss: 2.0580\n",
      "Epoch: 8/40... Step: 180... Loss: 1.8829... Val Loss: 2.0296\n",
      "Epoch: 8/40... Step: 190... Loss: 1.8553... Val Loss: 1.9770\n",
      "Epoch: 9/40... Step: 200... Loss: 1.9125... Val Loss: 1.9811\n",
      "Epoch: 9/40... Step: 210... Loss: 1.8064... Val Loss: 1.9711\n",
      "Epoch: 10/40... Step: 220... Loss: 1.7960... Val Loss: 1.9023\n",
      "Epoch: 10/40... Step: 230... Loss: 1.8180... Val Loss: 1.8736\n",
      "Epoch: 10/40... Step: 240... Loss: 1.7530... Val Loss: 1.8379\n",
      "Epoch: 11/40... Step: 250... Loss: 1.6964... Val Loss: 1.8267\n",
      "Epoch: 11/40... Step: 260... Loss: 1.7274... Val Loss: 1.7873\n",
      "Epoch: 12/40... Step: 270... Loss: 1.7117... Val Loss: 1.7473\n",
      "Epoch: 12/40... Step: 280... Loss: 1.7042... Val Loss: 1.7195\n",
      "Epoch: 13/40... Step: 290... Loss: 1.6602... Val Loss: 1.7070\n",
      "Epoch: 13/40... Step: 300... Loss: 1.6092... Val Loss: 1.7048\n",
      "Epoch: 13/40... Step: 310... Loss: 1.6108... Val Loss: 1.6766\n",
      "Epoch: 14/40... Step: 320... Loss: 1.6340... Val Loss: 1.6647\n",
      "Epoch: 14/40... Step: 330... Loss: 1.5708... Val Loss: 1.6584\n",
      "Epoch: 15/40... Step: 340... Loss: 1.5818... Val Loss: 1.6327\n",
      "Epoch: 15/40... Step: 350... Loss: 1.6152... Val Loss: 1.6554\n",
      "Epoch: 15/40... Step: 360... Loss: 1.5672... Val Loss: 1.6491\n",
      "Epoch: 16/40... Step: 370... Loss: 1.5116... Val Loss: 1.6304\n",
      "Epoch: 16/40... Step: 380... Loss: 1.5341... Val Loss: 1.6651\n",
      "Epoch: 17/40... Step: 390... Loss: 1.5436... Val Loss: 1.6533\n",
      "Epoch: 17/40... Step: 400... Loss: 1.5279... Val Loss: 1.5690\n",
      "Epoch: 18/40... Step: 410... Loss: 1.5048... Val Loss: 1.6511\n",
      "Epoch: 18/40... Step: 420... Loss: 1.4428... Val Loss: 1.6350\n",
      "Epoch: 18/40... Step: 430... Loss: 1.4504... Val Loss: 1.6218\n",
      "Epoch: 19/40... Step: 440... Loss: 1.4811... Val Loss: 1.6086\n",
      "Epoch: 19/40... Step: 450... Loss: 1.4085... Val Loss: 1.6204\n",
      "Epoch: 20/40... Step: 460... Loss: 1.4263... Val Loss: 1.6038\n",
      "Epoch: 20/40... Step: 470... Loss: 1.4616... Val Loss: 1.6308\n",
      "Epoch: 20/40... Step: 480... Loss: 1.4269... Val Loss: 1.5999\n",
      "Epoch: 21/40... Step: 490... Loss: 1.3833... Val Loss: 1.6836\n",
      "Epoch: 21/40... Step: 500... Loss: 1.3993... Val Loss: 1.5899\n",
      "Epoch: 22/40... Step: 510... Loss: 1.4069... Val Loss: 1.5616\n",
      "Epoch: 22/40... Step: 520... Loss: 1.3892... Val Loss: 1.5858\n",
      "Epoch: 23/40... Step: 530... Loss: 1.3773... Val Loss: 1.5928\n",
      "Epoch: 23/40... Step: 540... Loss: 1.3160... Val Loss: 1.6033\n",
      "Epoch: 23/40... Step: 550... Loss: 1.3233... Val Loss: 1.6561\n",
      "Epoch: 24/40... Step: 560... Loss: 1.3505... Val Loss: 1.5806\n",
      "Epoch: 24/40... Step: 570... Loss: 1.2941... Val Loss: 1.6135\n",
      "Epoch: 25/40... Step: 580... Loss: 1.3052... Val Loss: 1.6193\n",
      "Epoch: 25/40... Step: 590... Loss: 1.3415... Val Loss: 1.6077\n",
      "Epoch: 25/40... Step: 600... Loss: 1.3168... Val Loss: 1.6149\n",
      "Epoch: 26/40... Step: 610... Loss: 1.2630... Val Loss: 1.6392\n",
      "Epoch: 26/40... Step: 620... Loss: 1.2802... Val Loss: 1.6564\n",
      "Epoch: 27/40... Step: 630... Loss: 1.3021... Val Loss: 1.5481\n",
      "Epoch: 27/40... Step: 640... Loss: 1.2782... Val Loss: 1.6219\n",
      "Epoch: 28/40... Step: 650... Loss: 1.2713... Val Loss: 1.5806\n",
      "Epoch: 28/40... Step: 660... Loss: 1.2089... Val Loss: 1.6447\n",
      "Epoch: 28/40... Step: 670... Loss: 1.2216... Val Loss: 1.6237\n",
      "Epoch: 29/40... Step: 680... Loss: 1.2453... Val Loss: 1.6277\n",
      "Epoch: 29/40... Step: 690... Loss: 1.1980... Val Loss: 1.6129\n",
      "Epoch: 30/40... Step: 700... Loss: 1.2122... Val Loss: 1.6564\n",
      "Epoch: 30/40... Step: 710... Loss: 1.2404... Val Loss: 1.6001\n",
      "Epoch: 30/40... Step: 720... Loss: 1.2263... Val Loss: 1.6696\n",
      "Epoch: 31/40... Step: 730... Loss: 1.1867... Val Loss: 1.5714\n",
      "Epoch: 31/40... Step: 740... Loss: 1.1855... Val Loss: 1.6008\n",
      "Epoch: 32/40... Step: 750... Loss: 1.2162... Val Loss: 1.5482\n",
      "Epoch: 32/40... Step: 760... Loss: 1.1882... Val Loss: 1.5544\n",
      "Epoch: 33/40... Step: 770... Loss: 1.1953... Val Loss: 1.6066\n",
      "Epoch: 33/40... Step: 780... Loss: 1.1245... Val Loss: 1.6171\n",
      "Epoch: 33/40... Step: 790... Loss: 1.1409... Val Loss: 1.6222\n",
      "Epoch: 34/40... Step: 800... Loss: 1.1732... Val Loss: 1.5792\n",
      "Epoch: 34/40... Step: 810... Loss: 1.1234... Val Loss: 1.5724\n",
      "Epoch: 35/40... Step: 820... Loss: 1.1317... Val Loss: 1.5913\n",
      "Epoch: 35/40... Step: 830... Loss: 1.1697... Val Loss: 1.6254\n",
      "Epoch: 35/40... Step: 840... Loss: 1.1544... Val Loss: 1.5886\n",
      "Epoch: 36/40... Step: 850... Loss: 1.1112... Val Loss: 1.6011\n",
      "Epoch: 36/40... Step: 860... Loss: 1.1148... Val Loss: 1.5771\n",
      "Epoch: 37/40... Step: 870... Loss: 1.1496... Val Loss: 1.5534\n",
      "Epoch: 37/40... Step: 880... Loss: 1.1153... Val Loss: 1.5918\n",
      "Epoch: 38/40... Step: 890... Loss: 1.1291... Val Loss: 1.6059\n",
      "Epoch: 38/40... Step: 900... Loss: 1.0622... Val Loss: 1.6112\n",
      "Epoch: 38/40... Step: 910... Loss: 1.0799... Val Loss: 1.5864\n",
      "Epoch: 39/40... Step: 920... Loss: 1.1127... Val Loss: 1.5356\n",
      "Epoch: 39/40... Step: 930... Loss: 1.0604... Val Loss: 1.5889\n",
      "Epoch: 40/40... Step: 940... Loss: 1.0855... Val Loss: 1.5448\n",
      "Epoch: 40/40... Step: 950... Loss: 1.1100... Val Loss: 1.5924\n",
      "Epoch: 40/40... Step: 960... Loss: 1.1090... Val Loss: 1.5652\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    net,\n",
    "    encoded_text,\n",
    "    epochs=40,\n",
    "    n_seqs=128,\n",
    "    n_steps=300,\n",
    "    lr=0.001,\n",
    "    cuda=torch.cuda.is_available(),\n",
    "    print_every=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGi6nN03ycGC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After training, we'll save the model so we can load it again later if we need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "08NxMviGycGC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict()}\n",
    "\n",
    "with open('trained_model.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMIfAIVzycGC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Sampling\n",
    "\n",
    "To sample from the trained model, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. By keeping doing this we'll generate a bunch of text.\n",
    "\n",
    "### Top K sampling\n",
    "\n",
    "Our predictions come from a categorical probability distribution over all the possible characters. We can make the sample text and make it more reasonable to handle (with less variables) by only considering some $K$ most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text.\n",
    "\n",
    "In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "gbzwujN-ycGC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_sample(net, sample_length, prime='<div>', top_k=None, cuda=False):\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    \n",
    "    h = net.init_hidden(1)\n",
    "    \n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    for ii in range(sample_length):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "U7sL3pHUycGC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "32591ddb-18eb-47f0-ac42-8e5367300b3d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<div>\n",
      "    < plane the problem this comparace found at takes a lens of the circuit water the surface is a foun its metion</i>. The current is called <i>a sparts.</p>\n",
      "    <p>4. Even a compated of which wave are parallel to a solution follow. </span>\n",
      "    </div>\n",
      "    <p>\n",
      "      <b>251. Second and Strick as second.</p>\n",
      "    <p>2. Charge is comnons of an electric connection is to resist action of to electric thater its supported in the seen. A following the cubic fool, as the condens that of the sents are considured by the earth this in arrage indicates that the energy of the solid of the lows. They at the effect of warming is called the <i>precentical equal</i> and <span class=\"pagenum\">\n",
      "        <a name=\"Page_324\" id=\"Page_333\">[Pg 334]</a>\n",
      "      </span>\n",
      "    </p>\n",
      "    <p>3. <span class=\"pagenum\">\n",
      "        <a name=\"Page_412\" id=\"Page_242\">[Pg 242]</a>\n",
      "      </span>\n",
      "    </p>\n",
      "    <p>2. </p>\n",
      "    <div class=\"figcenter\" style=\"width: 400px;\">\n",
      "      <img src=\"images/i_040.png\" width=\"250\" height=\"257\" alt=\"\" />\n",
      "      <span class=\"caption\">\n",
      "        <span class=\"smcap\">Fig. 140.</span>&mdash;The resilating the center of which it induced this ares insided to parallel the same of the seconds, when the current in the sound water that the energy coloring a condentor, as to the surface in the color is strunt on which a tell arranged the some papper to the temperature, when the steam of the armanure center of the coil. This propertanes water through a cold in the current in strange or stect of all condinades are three the consider by the strings, the sound it and the acrea transformation of the surfaces. </span>\n",
      "    </div>\n",
      "    <h3>\n",
      "      <a name=\"CHAPTER_XVI\" id=\"CHAPTER_XVIII\">CHAPTER XVIII</a>\n",
      "    </h2>\n",
      "    <p class=\"center\">TOR Ind Production Crassion.</b>&mdash;A current of senter, the end of a corfend of that word is studyed in orcensist and of the center. The receiver of a magnetic inselvents the resistance of thes word distontains wether when the latter tests and someters are connected in thiss place that the surface of a magnet is pointed aromether the straight is conditions to a lang of water and the stronger, the composton to the an ormation to the weight of a leng is at one at an ender or the same temperature? We cande of water a contannout in the sound of the laws the second it of each at the sound that the southented that the subface of alcen and coment one was is sence that the resonally sen the consition from the weight. </p>\n",
      "    <p>The stream of water. </span>\n",
      "    </div>\n",
      "    <div class=\"figleft\" style=\"width: 200px;\">\n",
      "      <img src=\"images/i_202.png\" width=\"200\" height=\"179\" alt=\"\" />\n",
      "      <span class=\"caption\">\n",
      "        <span class=\"smcap\">Fig. 218.</span>&mdash;The magnetoscope,, stroke in a magnetic fill and the surface that a strong its friction is supporded in second.</p>\n",
      "    <p>1. The ratio of the refrecting computity a devalon and charges of a liquid in a larking coil, the called as indicate travels of the curven if a barken those are called.</p>\n",
      "    <p>3. Charging the same of a call between and the car of a sora pipch, a consider to at there at <i>O</i>. <i>The current.</p>\n",
      "    <p>10. The center of water in some in the and that the connecter of matures. </span>\n",
      "    </div>\n",
      "    <p>\n",
      "      <span class=\"pagenum\">\n",
      "        <a name=\"Page_218\" id=\"Page_222\">[Pg 222]</a>\n",
      "      </span> the surface of stending is also acterity, the earth. </span>\n",
      "    </div>\n",
      "    <p>\n",
      "      <i>A thos in the current the strong is a place to at and if into the are current, or a condinurally cansited in the change of support. (See Fig. 123.) If which as in the surforn or a steam of the acter than in the power. The current from the carrys is contented by the frece in other at would a cond take of the sormon are are any to carres one and sound and shidt at the colds, this force of the action of the surface of this current to resistance of steed of the cells is seen in simele of a coil of the pressure and it in a convacure of seching water wheel and the same position of\n"
     ]
    }
   ],
   "source": [
    "sample = generate_sample(net, 4000, top_k=5, cuda=True)\n",
    "\n",
    "print(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "colab": {
   "provenance": []
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}