{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENdpIzLyycFw"
      },
      "source": [
        "# Character-level LSTM with PyTorch\n",
        "\n",
        "An RNN model will be trained to generate new text character by character. Both the text used for training and the generated text will be analyzed with some statistical functions (e.g., for assessing readability), so that we can evaluate the statistical similarity between the training data and the generated sample."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY7R0wQFBgpN",
        "outputId": "9b25e7da-fe47-489e-ecb3-1fc462e5ec42"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[K     |████████████████████████████████| 105 kB 43.8 MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.13.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 48.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.13.2 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tsoK1d67ycF1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import textstat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_text_statistics(text):\n",
        "  return {\n",
        "      'Flesch reading ease': textstat.flesch_reading_ease(text),\n",
        "      'Linsear-Write Formula': textstat.linsear_write_formula(text),\n",
        "      'Dale-Chall readability score': textstat.dale_chall_readability_score(text),\n",
        "  }"
      ],
      "metadata": {
        "id": "uN9TKMNSjNTP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JewJr-vMycF2"
      },
      "source": [
        "## Training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DP6f7VrkycF2"
      },
      "outputs": [],
      "source": [
        "with open('shakespeare.txt', 'r') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_text_statistics(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nVD5jnMjh1G",
        "outputId": "356b1798-207f-4749-eed8-886c6d0898dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Flesch reading ease': 88.57,\n",
              " 'Linsear-Write Formula': 4.636363636363637,\n",
              " 'Dale-Chall readability score': 1.26}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-aoQoHIycF3"
      },
      "source": [
        "### Characters tokenization\n",
        "\n",
        "We need to be able to convert each character into an integer token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_ylyruewycF3"
      },
      "outputs": [],
      "source": [
        "chars_in_text = tuple(set(text))\n",
        "\n",
        "int2char = dict(enumerate(chars_in_text))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "\n",
        "encoded_text = np.array([char2int[ch] for ch in text])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding and then decoding produces the initial character\n",
        "print(int2char[char2int['X']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsTcQHbOmIXW",
        "outputId": "02e0a652-680a-45a5-a8ae-776c098c145e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naZeJhL6ycF6"
      },
      "source": [
        "The LSTM accepts only one-hot encoded vectors, so we prepare a function that takes as input an array of encoded characters and outputs an array of one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Lyerrn0jycF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e2a464-e790-461f-818e-01019a69f5ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "def to_one_hot(char_encoding):\n",
        "    one_hot = np.zeros(len(int2char))\n",
        "    one_hot[char_encoding] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Test\n",
        "to_one_hot(12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0fYgSq2ycF7"
      },
      "source": [
        "## Making training mini-batches\n",
        "\n",
        "We want to create both the input and target arrays, with the targets being the same as the inputs, but shifted over one character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SPrN0R-aycF8"
      },
      "outputs": [],
      "source": [
        "# Divide the `encodings_arr` array into batches.\n",
        "def get_batches(encodings_arr, sequences_per_batch, chars_per_sequence):\n",
        "    batch_size = sequences_per_batch * chars_per_sequence\n",
        "    number_of_batches = len(encodings_arr)//batch_size\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    encodings_arr = encodings_arr[:number_of_batches * batch_size]\n",
        "    \n",
        "    # Split the array into sequences by reshaping it into `sequences_per_batch` rows.\n",
        "    encodings_arr = encodings_arr.reshape((sequences_per_batch, -1))\n",
        "    \n",
        "    for n in range(0, encodings_arr.shape[1], chars_per_sequence):        \n",
        "        # The features\n",
        "        x = encodings_arr[:, n:n+chars_per_sequence]\n",
        "        \n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        \n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], encodings_arr[:, n+chars_per_sequence]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], encodings_arr[:, 0]\n",
        "        \n",
        "        yield x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test the mini-batches generation"
      ],
      "metadata": {
        "id": "UVfu3TPeuPlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_batch = next(get_batches(encoded_text, 3, 100))\n",
        "print(f'Every mini-batch is a tuple of {len(first_batch)} arrays.')\n",
        "print(f'First one contains features and in this case has shape {first_batch[0].shape}')\n",
        "print(f'Second one contains targets and in this case has shape {first_batch[1].shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RV7g3posJ77",
        "outputId": "613f6157-e849-4e78-8104-a0ee599b9056"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every mini-batch is a tuple of 2 arrays.\n",
            "First one contains features and in this case has shape (3, 100)\n",
            "Second one contains targets and in this case has shape (3, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uacfp0MMycF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2634f7c4-4192-4ed2-bd5d-97d465fe6c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x\n",
            " [[51 21 55  9  3 20  8 21  3 21]\n",
            " [20 29 20 24 16  4 23 20 16 45]\n",
            " [ 3 55  7 63 52 20 16  9 20  3]]\n",
            "\n",
            "y\n",
            " [[21 55  9  3 20  8 21  3 21  6]\n",
            " [29 20 24 16  4 23 20 16 45 20]\n",
            " [55  7 63 52 20 16  9 20  3  0]]\n"
          ]
        }
      ],
      "source": [
        "x, y = first_batch\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5NJ0QTRycF9"
      },
      "source": [
        "The data is correctly shifted over one step for `y`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65etXbJ0ycF9"
      },
      "source": [
        "---\n",
        "## Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PoanYVyPycF-"
      },
      "outputs": [],
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "\n",
        "        char_embedding_size = len(int2char) # size of the one-hot vectors representing characters\n",
        "        self.lstm = nn.LSTM(char_embedding_size, n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        self.fc = nn.Linear(n_hidden, char_embedding_size)\n",
        "        \n",
        "        self.init_weights()\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hc):\n",
        "        # Get x, and the new hidden state (h, c) from the lstm\n",
        "        x, (h, c) = self.lstm(x, hc)\n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Stack up LSTM outputs\n",
        "        x = x.reshape(x.size()[0] * x.size()[1], self.n_hidden)\n",
        "        \n",
        "        x = self.fc(x)\n",
        "        \n",
        "        # Return x and the hidden state (h, c)\n",
        "        return x, (h, c)\n",
        "    \n",
        "    \n",
        "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
        "        ''' Given a character and the hidden state, predict the next character.\n",
        "        \n",
        "            Returns the predicted character and the new hidden state.\n",
        "        '''\n",
        "        if cuda:\n",
        "            self.cuda()\n",
        "        else:\n",
        "            self.cpu()\n",
        "        \n",
        "        if h is None:\n",
        "            h = self.init_hidden(1)\n",
        "        \n",
        "        x = np.array([[to_one_hot(char2int[char])]], dtype=np.float32)\n",
        "        \n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        h = tuple([h_item.data for h_item in h])\n",
        "        out, h = self.forward(inputs, h)\n",
        "\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        \n",
        "        if cuda:\n",
        "            p = p.cpu()\n",
        "        \n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(int2char))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        p = p.numpy().squeeze()\n",
        "        \n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "            \n",
        "        return int2char[char], h\n",
        "    \n",
        "    def init_weights(self):\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-1, 1)\n",
        "        \n",
        "    def init_hidden(self, n_seqs):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XWWejkPycF_"
      },
      "source": [
        "## Network training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bOsuboWKycF_"
      },
      "outputs": [],
      "source": [
        "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
        "        n_steps: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        cuda: Train with CUDA on a GPU\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if cuda:\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        \n",
        "        h = net.init_hidden(n_seqs)\n",
        "        \n",
        "        for x, y in get_batches(data, n_seqs, n_steps):\n",
        "            \n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = np.array([\n",
        "                [to_one_hot(c) for c in seq] for seq in x\n",
        "            ], dtype=np.float32)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            net.zero_grad()\n",
        "            \n",
        "            output, h = net.forward(inputs, h)\n",
        "            \n",
        "            loss = criterion(\n",
        "                output,\n",
        "                targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor if cuda else torch.LongTensor)\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "\n",
        "            opt.step()\n",
        "            \n",
        "            if counter % print_every == 0:\n",
        "                \n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(n_seqs)\n",
        "                val_losses = []\n",
        "                \n",
        "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
        "                    \n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = np.array([\n",
        "                        [to_one_hot(c) for c in seq] for seq in x\n",
        "                    ], dtype=np.float32)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if cuda:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net.forward(inputs, val_h)\n",
        "                    val_loss = criterion(\n",
        "                        output,\n",
        "                        targets.view(n_seqs*n_steps).type(torch.cuda.LongTensor if cuda else torch.LongTensor)\n",
        "                    )\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7PX3uPSycGB",
        "outputId": "caf780a0-9e44-4002-afc1-84be01de9046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(67, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=67, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "net = CharRNN(n_hidden=512, n_layers=2)\n",
        "\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NpfNmlHycGB",
        "outputId": "205c2e2e-bb2b-46bc-e585-048007f484e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/25... Step: 10... Loss: 3.3878... Val Loss: 3.3914\n",
            "Epoch: 1/25... Step: 20... Loss: 3.2668... Val Loss: 3.2564\n",
            "Epoch: 1/25... Step: 30... Loss: 3.0851... Val Loss: 3.1035\n",
            "Epoch: 1/25... Step: 40... Loss: 2.9341... Val Loss: 2.9247\n",
            "Epoch: 1/25... Step: 50... Loss: 2.7599... Val Loss: 2.7579\n",
            "Epoch: 1/25... Step: 60... Loss: 2.6266... Val Loss: 2.6331\n",
            "Epoch: 1/25... Step: 70... Loss: 2.5490... Val Loss: 2.5572\n",
            "Epoch: 1/25... Step: 80... Loss: 2.4829... Val Loss: 2.5053\n",
            "Epoch: 1/25... Step: 90... Loss: 2.4587... Val Loss: 2.4642\n",
            "Epoch: 1/25... Step: 100... Loss: 2.4608... Val Loss: 2.4239\n",
            "Epoch: 1/25... Step: 110... Loss: 2.3936... Val Loss: 2.3976\n",
            "Epoch: 1/25... Step: 120... Loss: 2.3603... Val Loss: 2.3686\n",
            "Epoch: 1/25... Step: 130... Loss: 2.3143... Val Loss: 2.3466\n",
            "Epoch: 1/25... Step: 140... Loss: 2.3067... Val Loss: 2.3238\n",
            "Epoch: 1/25... Step: 150... Loss: 2.2630... Val Loss: 2.3039\n",
            "Epoch: 1/25... Step: 160... Loss: 2.2731... Val Loss: 2.2864\n",
            "Epoch: 1/25... Step: 170... Loss: 2.2487... Val Loss: 2.2672\n",
            "Epoch: 1/25... Step: 180... Loss: 2.2181... Val Loss: 2.2477\n",
            "Epoch: 1/25... Step: 190... Loss: 2.2126... Val Loss: 2.2352\n",
            "Epoch: 1/25... Step: 200... Loss: 2.1779... Val Loss: 2.2223\n",
            "Epoch: 1/25... Step: 210... Loss: 2.2034... Val Loss: 2.2128\n",
            "Epoch: 1/25... Step: 220... Loss: 2.1740... Val Loss: 2.1980\n",
            "Epoch: 1/25... Step: 230... Loss: 2.1548... Val Loss: 2.1802\n",
            "Epoch: 1/25... Step: 240... Loss: 2.1342... Val Loss: 2.1700\n",
            "Epoch: 1/25... Step: 250... Loss: 2.1234... Val Loss: 2.1562\n",
            "Epoch: 1/25... Step: 260... Loss: 2.1294... Val Loss: 2.1426\n",
            "Epoch: 1/25... Step: 270... Loss: 2.1149... Val Loss: 2.1308\n",
            "Epoch: 1/25... Step: 280... Loss: 2.1197... Val Loss: 2.2680\n",
            "Epoch: 1/25... Step: 290... Loss: 2.0703... Val Loss: 2.1073\n",
            "Epoch: 1/25... Step: 300... Loss: 2.0618... Val Loss: 2.1043\n",
            "Epoch: 1/25... Step: 310... Loss: 2.0438... Val Loss: 2.0902\n",
            "Epoch: 1/25... Step: 320... Loss: 2.0314... Val Loss: 2.0840\n",
            "Epoch: 2/25... Step: 330... Loss: 2.0162... Val Loss: 2.0702\n",
            "Epoch: 2/25... Step: 340... Loss: 2.0154... Val Loss: 2.0656\n",
            "Epoch: 2/25... Step: 350... Loss: 2.0134... Val Loss: 2.0457\n",
            "Epoch: 2/25... Step: 360... Loss: 2.0253... Val Loss: 2.0491\n",
            "Epoch: 2/25... Step: 370... Loss: 1.9492... Val Loss: 2.0330\n",
            "Epoch: 2/25... Step: 380... Loss: 1.9980... Val Loss: 2.0496\n",
            "Epoch: 2/25... Step: 390... Loss: 2.0030... Val Loss: 2.0289\n",
            "Epoch: 2/25... Step: 400... Loss: 1.9858... Val Loss: 2.0171\n",
            "Epoch: 2/25... Step: 410... Loss: 1.9479... Val Loss: 2.0119\n",
            "Epoch: 2/25... Step: 420... Loss: 1.9821... Val Loss: 1.9968\n",
            "Epoch: 2/25... Step: 430... Loss: 1.9823... Val Loss: 1.9934\n",
            "Epoch: 2/25... Step: 440... Loss: 1.9378... Val Loss: 1.9817\n",
            "Epoch: 2/25... Step: 450... Loss: 1.9272... Val Loss: 1.9773\n",
            "Epoch: 2/25... Step: 460... Loss: 1.9304... Val Loss: 1.9757\n",
            "Epoch: 2/25... Step: 470... Loss: 1.9350... Val Loss: 1.9675\n",
            "Epoch: 2/25... Step: 480... Loss: 1.9074... Val Loss: 1.9594\n",
            "Epoch: 2/25... Step: 490... Loss: 1.9097... Val Loss: 1.9524\n",
            "Epoch: 2/25... Step: 500... Loss: 1.8903... Val Loss: 1.9443\n",
            "Epoch: 2/25... Step: 510... Loss: 1.8982... Val Loss: 1.9416\n",
            "Epoch: 2/25... Step: 520... Loss: 1.8625... Val Loss: 1.9315\n",
            "Epoch: 2/25... Step: 530... Loss: 1.8674... Val Loss: 1.9328\n",
            "Epoch: 2/25... Step: 540... Loss: 1.8659... Val Loss: 1.9333\n",
            "Epoch: 2/25... Step: 550... Loss: 1.8603... Val Loss: 1.9237\n",
            "Epoch: 2/25... Step: 560... Loss: 1.8718... Val Loss: 1.9190\n",
            "Epoch: 2/25... Step: 570... Loss: 1.8405... Val Loss: 1.9090\n",
            "Epoch: 2/25... Step: 580... Loss: 1.8717... Val Loss: 1.8977\n",
            "Epoch: 2/25... Step: 590... Loss: 1.8328... Val Loss: 1.8952\n",
            "Epoch: 2/25... Step: 600... Loss: 1.8340... Val Loss: 1.8921\n",
            "Epoch: 2/25... Step: 610... Loss: 1.8284... Val Loss: 1.8919\n",
            "Epoch: 2/25... Step: 620... Loss: 1.8175... Val Loss: 1.8891\n",
            "Epoch: 2/25... Step: 630... Loss: 1.8112... Val Loss: 1.8830\n",
            "Epoch: 2/25... Step: 640... Loss: 1.8004... Val Loss: 1.8748\n",
            "Epoch: 3/25... Step: 650... Loss: 1.7927... Val Loss: 1.8762\n",
            "Epoch: 3/25... Step: 660... Loss: 1.7955... Val Loss: 1.8621\n",
            "Epoch: 3/25... Step: 670... Loss: 1.7819... Val Loss: 1.8619\n",
            "Epoch: 3/25... Step: 680... Loss: 1.7706... Val Loss: 1.8538\n",
            "Epoch: 3/25... Step: 690... Loss: 1.7636... Val Loss: 1.8539\n",
            "Epoch: 3/25... Step: 700... Loss: 1.7795... Val Loss: 1.8443\n",
            "Epoch: 3/25... Step: 710... Loss: 1.7722... Val Loss: 1.8437\n",
            "Epoch: 3/25... Step: 720... Loss: 1.7470... Val Loss: 1.8389\n",
            "Epoch: 3/25... Step: 730... Loss: 1.7472... Val Loss: 1.8303\n",
            "Epoch: 3/25... Step: 740... Loss: 1.7559... Val Loss: 1.8323\n",
            "Epoch: 3/25... Step: 750... Loss: 1.7499... Val Loss: 1.8268\n",
            "Epoch: 3/25... Step: 760... Loss: 1.7473... Val Loss: 1.8186\n",
            "Epoch: 3/25... Step: 770... Loss: 1.7271... Val Loss: 1.8173\n",
            "Epoch: 3/25... Step: 780... Loss: 1.7036... Val Loss: 1.8153\n",
            "Epoch: 3/25... Step: 790... Loss: 1.7198... Val Loss: 1.8137\n",
            "Epoch: 3/25... Step: 800... Loss: 1.7237... Val Loss: 1.8070\n",
            "Epoch: 3/25... Step: 810... Loss: 1.7585... Val Loss: 1.8026\n",
            "Epoch: 3/25... Step: 820... Loss: 1.7232... Val Loss: 1.7995\n",
            "Epoch: 3/25... Step: 830... Loss: 1.6923... Val Loss: 1.7980\n",
            "Epoch: 3/25... Step: 840... Loss: 1.6717... Val Loss: 1.7962\n",
            "Epoch: 3/25... Step: 850... Loss: 1.6976... Val Loss: 1.7959\n",
            "Epoch: 3/25... Step: 860... Loss: 1.7070... Val Loss: 1.7881\n",
            "Epoch: 3/25... Step: 870... Loss: 1.7104... Val Loss: 1.7964\n",
            "Epoch: 3/25... Step: 880... Loss: 1.6783... Val Loss: 1.7882\n",
            "Epoch: 3/25... Step: 890... Loss: 1.7253... Val Loss: 1.7843\n",
            "Epoch: 3/25... Step: 900... Loss: 1.6910... Val Loss: 1.7836\n",
            "Epoch: 3/25... Step: 910... Loss: 1.7112... Val Loss: 1.7729\n",
            "Epoch: 3/25... Step: 920... Loss: 1.6838... Val Loss: 1.7742\n",
            "Epoch: 3/25... Step: 930... Loss: 1.6613... Val Loss: 1.7719\n",
            "Epoch: 3/25... Step: 940... Loss: 1.6394... Val Loss: 1.7686\n",
            "Epoch: 3/25... Step: 950... Loss: 1.6314... Val Loss: 1.7662\n",
            "Epoch: 3/25... Step: 960... Loss: 1.6656... Val Loss: 1.7698\n",
            "Epoch: 4/25... Step: 970... Loss: 1.6518... Val Loss: 1.7676\n",
            "Epoch: 4/25... Step: 980... Loss: 1.6429... Val Loss: 1.7619\n",
            "Epoch: 4/25... Step: 990... Loss: 1.6820... Val Loss: 1.7548\n",
            "Epoch: 4/25... Step: 1000... Loss: 1.6773... Val Loss: 1.7535\n",
            "Epoch: 4/25... Step: 1010... Loss: 1.6256... Val Loss: 1.7501\n",
            "Epoch: 4/25... Step: 1020... Loss: 1.6258... Val Loss: 1.7448\n",
            "Epoch: 4/25... Step: 1030... Loss: 1.6483... Val Loss: 1.7379\n",
            "Epoch: 4/25... Step: 1040... Loss: 1.6499... Val Loss: 1.7431\n",
            "Epoch: 4/25... Step: 1050... Loss: 1.6439... Val Loss: 1.7336\n",
            "Epoch: 4/25... Step: 1060... Loss: 1.6526... Val Loss: 1.7393\n",
            "Epoch: 4/25... Step: 1070... Loss: 1.6324... Val Loss: 1.7385\n",
            "Epoch: 4/25... Step: 1080... Loss: 1.6344... Val Loss: 1.7333\n",
            "Epoch: 4/25... Step: 1090... Loss: 1.6350... Val Loss: 1.7322\n",
            "Epoch: 4/25... Step: 1100... Loss: 1.6314... Val Loss: 1.7278\n",
            "Epoch: 4/25... Step: 1110... Loss: 1.6163... Val Loss: 1.7254\n",
            "Epoch: 4/25... Step: 1120... Loss: 1.6208... Val Loss: 1.7209\n",
            "Epoch: 4/25... Step: 1130... Loss: 1.6254... Val Loss: 1.7228\n",
            "Epoch: 4/25... Step: 1140... Loss: 1.6234... Val Loss: 1.7122\n",
            "Epoch: 4/25... Step: 1150... Loss: 1.6172... Val Loss: 1.7130\n",
            "Epoch: 4/25... Step: 1160... Loss: 1.5920... Val Loss: 1.7130\n",
            "Epoch: 4/25... Step: 1170... Loss: 1.6360... Val Loss: 1.7147\n",
            "Epoch: 4/25... Step: 1180... Loss: 1.6095... Val Loss: 1.7124\n",
            "Epoch: 4/25... Step: 1190... Loss: 1.6268... Val Loss: 1.7128\n",
            "Epoch: 4/25... Step: 1200... Loss: 1.6066... Val Loss: 1.7091\n",
            "Epoch: 4/25... Step: 1210... Loss: 1.6141... Val Loss: 1.7100\n",
            "Epoch: 4/25... Step: 1220... Loss: 1.5919... Val Loss: 1.7016\n",
            "Epoch: 4/25... Step: 1230... Loss: 1.6187... Val Loss: 1.7030\n",
            "Epoch: 4/25... Step: 1240... Loss: 1.6113... Val Loss: 1.7036\n",
            "Epoch: 4/25... Step: 1250... Loss: 1.6190... Val Loss: 1.6971\n",
            "Epoch: 4/25... Step: 1260... Loss: 1.6026... Val Loss: 1.6943\n",
            "Epoch: 4/25... Step: 1270... Loss: 1.5783... Val Loss: 1.6922\n",
            "Epoch: 4/25... Step: 1280... Loss: 1.5715... Val Loss: 1.6928\n",
            "Epoch: 5/25... Step: 1290... Loss: 1.6049... Val Loss: 1.6953\n",
            "Epoch: 5/25... Step: 1300... Loss: 1.5913... Val Loss: 1.6870\n",
            "Epoch: 5/25... Step: 1310... Loss: 1.5901... Val Loss: 1.6836\n",
            "Epoch: 5/25... Step: 1320... Loss: 1.5834... Val Loss: 1.6841\n",
            "Epoch: 5/25... Step: 1330... Loss: 1.5730... Val Loss: 1.6818\n",
            "Epoch: 5/25... Step: 1340... Loss: 1.5570... Val Loss: 1.6846\n",
            "Epoch: 5/25... Step: 1350... Loss: 1.5813... Val Loss: 1.6800\n",
            "Epoch: 5/25... Step: 1360... Loss: 1.5647... Val Loss: 1.6774\n",
            "Epoch: 5/25... Step: 1370... Loss: 1.5731... Val Loss: 1.6751\n",
            "Epoch: 5/25... Step: 1380... Loss: 1.5771... Val Loss: 1.6754\n",
            "Epoch: 5/25... Step: 1390... Loss: 1.5751... Val Loss: 1.6734\n",
            "Epoch: 5/25... Step: 1400... Loss: 1.5904... Val Loss: 1.6699\n",
            "Epoch: 5/25... Step: 1410... Loss: 1.5888... Val Loss: 1.6648\n",
            "Epoch: 5/25... Step: 1420... Loss: 1.5357... Val Loss: 1.6689\n",
            "Epoch: 5/25... Step: 1430... Loss: 1.5321... Val Loss: 1.6725\n",
            "Epoch: 5/25... Step: 1440... Loss: 1.5537... Val Loss: 1.6630\n",
            "Epoch: 5/25... Step: 1450... Loss: 1.5939... Val Loss: 1.6614\n",
            "Epoch: 5/25... Step: 1460... Loss: 1.5477... Val Loss: 1.6574\n",
            "Epoch: 5/25... Step: 1470... Loss: 1.5520... Val Loss: 1.6668\n",
            "Epoch: 5/25... Step: 1480... Loss: 1.5273... Val Loss: 1.6645\n",
            "Epoch: 5/25... Step: 1490... Loss: 1.5266... Val Loss: 1.6594\n",
            "Epoch: 5/25... Step: 1500... Loss: 1.5506... Val Loss: 1.6634\n",
            "Epoch: 5/25... Step: 1510... Loss: 1.5252... Val Loss: 1.6588\n",
            "Epoch: 5/25... Step: 1520... Loss: 1.5437... Val Loss: 1.6557\n",
            "Epoch: 5/25... Step: 1530... Loss: 1.5790... Val Loss: 1.6537\n",
            "Epoch: 5/25... Step: 1540... Loss: 1.5422... Val Loss: 1.6547\n",
            "Epoch: 5/25... Step: 1550... Loss: 1.5511... Val Loss: 1.6546\n",
            "Epoch: 5/25... Step: 1560... Loss: 1.5334... Val Loss: 1.6571\n",
            "Epoch: 5/25... Step: 1570... Loss: 1.5490... Val Loss: 1.6552\n",
            "Epoch: 5/25... Step: 1580... Loss: 1.5523... Val Loss: 1.6514\n",
            "Epoch: 5/25... Step: 1590... Loss: 1.5213... Val Loss: 1.6481\n",
            "Epoch: 5/25... Step: 1600... Loss: 1.5384... Val Loss: 1.6477\n",
            "Epoch: 6/25... Step: 1610... Loss: 1.5142... Val Loss: 1.6410\n",
            "Epoch: 6/25... Step: 1620... Loss: 1.5361... Val Loss: 1.6408\n",
            "Epoch: 6/25... Step: 1630... Loss: 1.5418... Val Loss: 1.6525\n",
            "Epoch: 6/25... Step: 1640... Loss: 1.5594... Val Loss: 1.6407\n",
            "Epoch: 6/25... Step: 1650... Loss: 1.5075... Val Loss: 1.6417\n",
            "Epoch: 6/25... Step: 1660... Loss: 1.5134... Val Loss: 1.6356\n",
            "Epoch: 6/25... Step: 1670... Loss: 1.5222... Val Loss: 1.6349\n",
            "Epoch: 6/25... Step: 1680... Loss: 1.5066... Val Loss: 1.6329\n",
            "Epoch: 6/25... Step: 1690... Loss: 1.5151... Val Loss: 1.6400\n",
            "Epoch: 6/25... Step: 1700... Loss: 1.5289... Val Loss: 1.6369\n",
            "Epoch: 6/25... Step: 1710... Loss: 1.5131... Val Loss: 1.6394\n",
            "Epoch: 6/25... Step: 1720... Loss: 1.5431... Val Loss: 1.6308\n",
            "Epoch: 6/25... Step: 1730... Loss: 1.5218... Val Loss: 1.6334\n",
            "Epoch: 6/25... Step: 1740... Loss: 1.5186... Val Loss: 1.6283\n",
            "Epoch: 6/25... Step: 1750... Loss: 1.5238... Val Loss: 1.6280\n",
            "Epoch: 6/25... Step: 1760... Loss: 1.5076... Val Loss: 1.6232\n",
            "Epoch: 6/25... Step: 1770... Loss: 1.5576... Val Loss: 1.6271\n",
            "Epoch: 6/25... Step: 1780... Loss: 1.4902... Val Loss: 1.6189\n",
            "Epoch: 6/25... Step: 1790... Loss: 1.5175... Val Loss: 1.6272\n",
            "Epoch: 6/25... Step: 1800... Loss: 1.5107... Val Loss: 1.6258\n",
            "Epoch: 6/25... Step: 1810... Loss: 1.4917... Val Loss: 1.6263\n",
            "Epoch: 6/25... Step: 1820... Loss: 1.4859... Val Loss: 1.6298\n",
            "Epoch: 6/25... Step: 1830... Loss: 1.5038... Val Loss: 1.6200\n",
            "Epoch: 6/25... Step: 1840... Loss: 1.5259... Val Loss: 1.6208\n",
            "Epoch: 6/25... Step: 1850... Loss: 1.5366... Val Loss: 1.6222\n",
            "Epoch: 6/25... Step: 1860... Loss: 1.4942... Val Loss: 1.6231\n",
            "Epoch: 6/25... Step: 1870... Loss: 1.4790... Val Loss: 1.6222\n",
            "Epoch: 6/25... Step: 1880... Loss: 1.5001... Val Loss: 1.6226\n",
            "Epoch: 6/25... Step: 1890... Loss: 1.4621... Val Loss: 1.6219\n",
            "Epoch: 6/25... Step: 1900... Loss: 1.5033... Val Loss: 1.6246\n",
            "Epoch: 6/25... Step: 1910... Loss: 1.4920... Val Loss: 1.6137\n",
            "Epoch: 6/25... Step: 1920... Loss: 1.4834... Val Loss: 1.6172\n",
            "Epoch: 7/25... Step: 1930... Loss: 1.4972... Val Loss: 1.6127\n",
            "Epoch: 7/25... Step: 1940... Loss: 1.4677... Val Loss: 1.6152\n",
            "Epoch: 7/25... Step: 1950... Loss: 1.5063... Val Loss: 1.6181\n",
            "Epoch: 7/25... Step: 1960... Loss: 1.4863... Val Loss: 1.6066\n",
            "Epoch: 7/25... Step: 1970... Loss: 1.4960... Val Loss: 1.6074\n",
            "Epoch: 7/25... Step: 1980... Loss: 1.4371... Val Loss: 1.6070\n",
            "Epoch: 7/25... Step: 1990... Loss: 1.4659... Val Loss: 1.6061\n",
            "Epoch: 7/25... Step: 2000... Loss: 1.4738... Val Loss: 1.6003\n",
            "Epoch: 7/25... Step: 2010... Loss: 1.4442... Val Loss: 1.6070\n",
            "Epoch: 7/25... Step: 2020... Loss: 1.4879... Val Loss: 1.6028\n",
            "Epoch: 7/25... Step: 2030... Loss: 1.4679... Val Loss: 1.6085\n",
            "Epoch: 7/25... Step: 2040... Loss: 1.4954... Val Loss: 1.6046\n",
            "Epoch: 7/25... Step: 2050... Loss: 1.4854... Val Loss: 1.6079\n",
            "Epoch: 7/25... Step: 2060... Loss: 1.4607... Val Loss: 1.6028\n",
            "Epoch: 7/25... Step: 2070... Loss: 1.4801... Val Loss: 1.6055\n",
            "Epoch: 7/25... Step: 2080... Loss: 1.4565... Val Loss: 1.6019\n",
            "Epoch: 7/25... Step: 2090... Loss: 1.4539... Val Loss: 1.5954\n",
            "Epoch: 7/25... Step: 2100... Loss: 1.4599... Val Loss: 1.5942\n",
            "Epoch: 7/25... Step: 2110... Loss: 1.4445... Val Loss: 1.5923\n",
            "Epoch: 7/25... Step: 2120... Loss: 1.4763... Val Loss: 1.5900\n",
            "Epoch: 7/25... Step: 2130... Loss: 1.4313... Val Loss: 1.6004\n",
            "Epoch: 7/25... Step: 2140... Loss: 1.4533... Val Loss: 1.6035\n",
            "Epoch: 7/25... Step: 2150... Loss: 1.4660... Val Loss: 1.5900\n",
            "Epoch: 7/25... Step: 2160... Loss: 1.4823... Val Loss: 1.6028\n",
            "Epoch: 7/25... Step: 2170... Loss: 1.4355... Val Loss: 1.5921\n",
            "Epoch: 7/25... Step: 2180... Loss: 1.4597... Val Loss: 1.5972\n",
            "Epoch: 7/25... Step: 2190... Loss: 1.4472... Val Loss: 1.5936\n",
            "Epoch: 7/25... Step: 2200... Loss: 1.4692... Val Loss: 1.5903\n",
            "Epoch: 7/25... Step: 2210... Loss: 1.4630... Val Loss: 1.5920\n",
            "Epoch: 7/25... Step: 2220... Loss: 1.4520... Val Loss: 1.5906\n",
            "Epoch: 7/25... Step: 2230... Loss: 1.4439... Val Loss: 1.5921\n",
            "Epoch: 7/25... Step: 2240... Loss: 1.4740... Val Loss: 1.5862\n",
            "Epoch: 8/25... Step: 2250... Loss: 1.4358... Val Loss: 1.5875\n",
            "Epoch: 8/25... Step: 2260... Loss: 1.4423... Val Loss: 1.5901\n",
            "Epoch: 8/25... Step: 2270... Loss: 1.4621... Val Loss: 1.5892\n",
            "Epoch: 8/25... Step: 2280... Loss: 1.4233... Val Loss: 1.5787\n",
            "Epoch: 8/25... Step: 2290... Loss: 1.4250... Val Loss: 1.5832\n",
            "Epoch: 8/25... Step: 2300... Loss: 1.4127... Val Loss: 1.5812\n",
            "Epoch: 8/25... Step: 2310... Loss: 1.4432... Val Loss: 1.5781\n",
            "Epoch: 8/25... Step: 2320... Loss: 1.4687... Val Loss: 1.5809\n",
            "Epoch: 8/25... Step: 2330... Loss: 1.4544... Val Loss: 1.5886\n",
            "Epoch: 8/25... Step: 2340... Loss: 1.4244... Val Loss: 1.5785\n",
            "Epoch: 8/25... Step: 2350... Loss: 1.4480... Val Loss: 1.5826\n",
            "Epoch: 8/25... Step: 2360... Loss: 1.4497... Val Loss: 1.5875\n",
            "Epoch: 8/25... Step: 2370... Loss: 1.4613... Val Loss: 1.5811\n",
            "Epoch: 8/25... Step: 2380... Loss: 1.4527... Val Loss: 1.5805\n",
            "Epoch: 8/25... Step: 2390... Loss: 1.4608... Val Loss: 1.5885\n",
            "Epoch: 8/25... Step: 2400... Loss: 1.4362... Val Loss: 1.5807\n",
            "Epoch: 8/25... Step: 2410... Loss: 1.4562... Val Loss: 1.5720\n",
            "Epoch: 8/25... Step: 2420... Loss: 1.3956... Val Loss: 1.5787\n",
            "Epoch: 8/25... Step: 2430... Loss: 1.4263... Val Loss: 1.5740\n",
            "Epoch: 8/25... Step: 2440... Loss: 1.4425... Val Loss: 1.5719\n",
            "Epoch: 8/25... Step: 2450... Loss: 1.4368... Val Loss: 1.5759\n",
            "Epoch: 8/25... Step: 2460... Loss: 1.4361... Val Loss: 1.5780\n",
            "Epoch: 8/25... Step: 2470... Loss: 1.4723... Val Loss: 1.5654\n",
            "Epoch: 8/25... Step: 2480... Loss: 1.4386... Val Loss: 1.5765\n",
            "Epoch: 8/25... Step: 2490... Loss: 1.4484... Val Loss: 1.5744\n",
            "Epoch: 8/25... Step: 2500... Loss: 1.4115... Val Loss: 1.5684\n",
            "Epoch: 8/25... Step: 2510... Loss: 1.4364... Val Loss: 1.5695\n",
            "Epoch: 8/25... Step: 2520... Loss: 1.4261... Val Loss: 1.5667\n",
            "Epoch: 8/25... Step: 2530... Loss: 1.4162... Val Loss: 1.5745\n",
            "Epoch: 8/25... Step: 2540... Loss: 1.4405... Val Loss: 1.5667\n",
            "Epoch: 8/25... Step: 2550... Loss: 1.4463... Val Loss: 1.5731\n",
            "Epoch: 8/25... Step: 2560... Loss: 1.4220... Val Loss: 1.5675\n",
            "Epoch: 9/25... Step: 2570... Loss: 1.3935... Val Loss: 1.5593\n",
            "Epoch: 9/25... Step: 2580... Loss: 1.4008... Val Loss: 1.5710\n",
            "Epoch: 9/25... Step: 2590... Loss: 1.4279... Val Loss: 1.5620\n",
            "Epoch: 9/25... Step: 2600... Loss: 1.4037... Val Loss: 1.5624\n",
            "Epoch: 9/25... Step: 2610... Loss: 1.4116... Val Loss: 1.5590\n",
            "Epoch: 9/25... Step: 2620... Loss: 1.4285... Val Loss: 1.5654\n",
            "Epoch: 9/25... Step: 2630... Loss: 1.4107... Val Loss: 1.5601\n",
            "Epoch: 9/25... Step: 2640... Loss: 1.3985... Val Loss: 1.5609\n",
            "Epoch: 9/25... Step: 2650... Loss: 1.4126... Val Loss: 1.5559\n",
            "Epoch: 9/25... Step: 2660... Loss: 1.3832... Val Loss: 1.5582\n",
            "Epoch: 9/25... Step: 2670... Loss: 1.4112... Val Loss: 1.5588\n",
            "Epoch: 9/25... Step: 2680... Loss: 1.4148... Val Loss: 1.5621\n",
            "Epoch: 9/25... Step: 2690... Loss: 1.4348... Val Loss: 1.5544\n",
            "Epoch: 9/25... Step: 2700... Loss: 1.3920... Val Loss: 1.5556\n",
            "Epoch: 9/25... Step: 2710... Loss: 1.4382... Val Loss: 1.5578\n",
            "Epoch: 9/25... Step: 2720... Loss: 1.4081... Val Loss: 1.5559\n",
            "Epoch: 9/25... Step: 2730... Loss: 1.4410... Val Loss: 1.5481\n",
            "Epoch: 9/25... Step: 2740... Loss: 1.4062... Val Loss: 1.5590\n",
            "Epoch: 9/25... Step: 2750... Loss: 1.4087... Val Loss: 1.5492\n",
            "Epoch: 9/25... Step: 2760... Loss: 1.4171... Val Loss: 1.5549\n",
            "Epoch: 9/25... Step: 2770... Loss: 1.4132... Val Loss: 1.5529\n",
            "Epoch: 9/25... Step: 2780... Loss: 1.3991... Val Loss: 1.5626\n",
            "Epoch: 9/25... Step: 2790... Loss: 1.4100... Val Loss: 1.5553\n",
            "Epoch: 9/25... Step: 2800... Loss: 1.4345... Val Loss: 1.5598\n",
            "Epoch: 9/25... Step: 2810... Loss: 1.4329... Val Loss: 1.5650\n",
            "Epoch: 9/25... Step: 2820... Loss: 1.4245... Val Loss: 1.5559\n",
            "Epoch: 9/25... Step: 2830... Loss: 1.4239... Val Loss: 1.5549\n",
            "Epoch: 9/25... Step: 2840... Loss: 1.4028... Val Loss: 1.5520\n",
            "Epoch: 9/25... Step: 2850... Loss: 1.4214... Val Loss: 1.5532\n",
            "Epoch: 9/25... Step: 2860... Loss: 1.3943... Val Loss: 1.5508\n",
            "Epoch: 9/25... Step: 2870... Loss: 1.3757... Val Loss: 1.5551\n",
            "Epoch: 9/25... Step: 2880... Loss: 1.4018... Val Loss: 1.5448\n",
            "Epoch: 10/25... Step: 2890... Loss: 1.4694... Val Loss: 1.5481\n",
            "Epoch: 10/25... Step: 2900... Loss: 1.3846... Val Loss: 1.5556\n",
            "Epoch: 10/25... Step: 2910... Loss: 1.3867... Val Loss: 1.5440\n",
            "Epoch: 10/25... Step: 2920... Loss: 1.3973... Val Loss: 1.5500\n",
            "Epoch: 10/25... Step: 2930... Loss: 1.3867... Val Loss: 1.5476\n",
            "Epoch: 10/25... Step: 2940... Loss: 1.3928... Val Loss: 1.5463\n",
            "Epoch: 10/25... Step: 2950... Loss: 1.3719... Val Loss: 1.5430\n",
            "Epoch: 10/25... Step: 2960... Loss: 1.4030... Val Loss: 1.5424\n",
            "Epoch: 10/25... Step: 2970... Loss: 1.4029... Val Loss: 1.5469\n",
            "Epoch: 10/25... Step: 2980... Loss: 1.3998... Val Loss: 1.5425\n",
            "Epoch: 10/25... Step: 2990... Loss: 1.4111... Val Loss: 1.5410\n",
            "Epoch: 10/25... Step: 3000... Loss: 1.3945... Val Loss: 1.5422\n",
            "Epoch: 10/25... Step: 3010... Loss: 1.3918... Val Loss: 1.5411\n",
            "Epoch: 10/25... Step: 3020... Loss: 1.3902... Val Loss: 1.5407\n",
            "Epoch: 10/25... Step: 3030... Loss: 1.4109... Val Loss: 1.5459\n",
            "Epoch: 10/25... Step: 3040... Loss: 1.4079... Val Loss: 1.5416\n",
            "Epoch: 10/25... Step: 3050... Loss: 1.4058... Val Loss: 1.5368\n",
            "Epoch: 10/25... Step: 3060... Loss: 1.3993... Val Loss: 1.5424\n",
            "Epoch: 10/25... Step: 3070... Loss: 1.4041... Val Loss: 1.5358\n",
            "Epoch: 10/25... Step: 3080... Loss: 1.4296... Val Loss: 1.5462\n",
            "Epoch: 10/25... Step: 3090... Loss: 1.3590... Val Loss: 1.5403\n",
            "Epoch: 10/25... Step: 3100... Loss: 1.4078... Val Loss: 1.5494\n",
            "Epoch: 10/25... Step: 3110... Loss: 1.4160... Val Loss: 1.5502\n",
            "Epoch: 10/25... Step: 3120... Loss: 1.4073... Val Loss: 1.5458\n",
            "Epoch: 10/25... Step: 3130... Loss: 1.4262... Val Loss: 1.5484\n",
            "Epoch: 10/25... Step: 3140... Loss: 1.3713... Val Loss: 1.5437\n",
            "Epoch: 10/25... Step: 3150... Loss: 1.3886... Val Loss: 1.5459\n",
            "Epoch: 10/25... Step: 3160... Loss: 1.3795... Val Loss: 1.5379\n",
            "Epoch: 10/25... Step: 3170... Loss: 1.3819... Val Loss: 1.5357\n",
            "Epoch: 10/25... Step: 3180... Loss: 1.3643... Val Loss: 1.5360\n",
            "Epoch: 10/25... Step: 3190... Loss: 1.3511... Val Loss: 1.5365\n",
            "Epoch: 10/25... Step: 3200... Loss: 1.4016... Val Loss: 1.5330\n",
            "Epoch: 10/25... Step: 3210... Loss: 1.4707... Val Loss: 1.5290\n",
            "Epoch: 11/25... Step: 3220... Loss: 1.3664... Val Loss: 1.5392\n",
            "Epoch: 11/25... Step: 3230... Loss: 1.4062... Val Loss: 1.5289\n",
            "Epoch: 11/25... Step: 3240... Loss: 1.3599... Val Loss: 1.5361\n",
            "Epoch: 11/25... Step: 3250... Loss: 1.3641... Val Loss: 1.5318\n",
            "Epoch: 11/25... Step: 3260... Loss: 1.3623... Val Loss: 1.5285\n",
            "Epoch: 11/25... Step: 3270... Loss: 1.3683... Val Loss: 1.5281\n",
            "Epoch: 11/25... Step: 3280... Loss: 1.3819... Val Loss: 1.5278\n",
            "Epoch: 11/25... Step: 3290... Loss: 1.3862... Val Loss: 1.5280\n",
            "Epoch: 11/25... Step: 3300... Loss: 1.3638... Val Loss: 1.5312\n",
            "Epoch: 11/25... Step: 3310... Loss: 1.4045... Val Loss: 1.5252\n",
            "Epoch: 11/25... Step: 3320... Loss: 1.4242... Val Loss: 1.5266\n",
            "Epoch: 11/25... Step: 3330... Loss: 1.3635... Val Loss: 1.5334\n",
            "Epoch: 11/25... Step: 3340... Loss: 1.3549... Val Loss: 1.5218\n",
            "Epoch: 11/25... Step: 3350... Loss: 1.3785... Val Loss: 1.5291\n",
            "Epoch: 11/25... Step: 3360... Loss: 1.3787... Val Loss: 1.5281\n",
            "Epoch: 11/25... Step: 3370... Loss: 1.3659... Val Loss: 1.5227\n",
            "Epoch: 11/25... Step: 3380... Loss: 1.3574... Val Loss: 1.5263\n",
            "Epoch: 11/25... Step: 3390... Loss: 1.3454... Val Loss: 1.5239\n",
            "Epoch: 11/25... Step: 3400... Loss: 1.3587... Val Loss: 1.5246\n",
            "Epoch: 11/25... Step: 3410... Loss: 1.3200... Val Loss: 1.5343\n",
            "Epoch: 11/25... Step: 3420... Loss: 1.4059... Val Loss: 1.5234\n",
            "Epoch: 11/25... Step: 3430... Loss: 1.3716... Val Loss: 1.5315\n",
            "Epoch: 11/25... Step: 3440... Loss: 1.3831... Val Loss: 1.5306\n",
            "Epoch: 11/25... Step: 3450... Loss: 1.3747... Val Loss: 1.5273\n",
            "Epoch: 11/25... Step: 3460... Loss: 1.3558... Val Loss: 1.5293\n",
            "Epoch: 11/25... Step: 3470... Loss: 1.3948... Val Loss: 1.5272\n",
            "Epoch: 11/25... Step: 3480... Loss: 1.3988... Val Loss: 1.5244\n",
            "Epoch: 11/25... Step: 3490... Loss: 1.3841... Val Loss: 1.5244\n",
            "Epoch: 11/25... Step: 3500... Loss: 1.3693... Val Loss: 1.5239\n",
            "Epoch: 11/25... Step: 3510... Loss: 1.3637... Val Loss: 1.5266\n",
            "Epoch: 11/25... Step: 3520... Loss: 1.3571... Val Loss: 1.5216\n",
            "Epoch: 11/25... Step: 3530... Loss: 1.3409... Val Loss: 1.5265\n",
            "Epoch: 12/25... Step: 3540... Loss: 1.3658... Val Loss: 1.5234\n",
            "Epoch: 12/25... Step: 3550... Loss: 1.3624... Val Loss: 1.5245\n",
            "Epoch: 12/25... Step: 3560... Loss: 1.3579... Val Loss: 1.5197\n",
            "Epoch: 12/25... Step: 3570... Loss: 1.3736... Val Loss: 1.5191\n",
            "Epoch: 12/25... Step: 3580... Loss: 1.3276... Val Loss: 1.5213\n",
            "Epoch: 12/25... Step: 3590... Loss: 1.3496... Val Loss: 1.5205\n",
            "Epoch: 12/25... Step: 3600... Loss: 1.3527... Val Loss: 1.5197\n",
            "Epoch: 12/25... Step: 3610... Loss: 1.3617... Val Loss: 1.5248\n",
            "Epoch: 12/25... Step: 3620... Loss: 1.3395... Val Loss: 1.5216\n",
            "Epoch: 12/25... Step: 3630... Loss: 1.3877... Val Loss: 1.5208\n",
            "Epoch: 12/25... Step: 3640... Loss: 1.4000... Val Loss: 1.5229\n",
            "Epoch: 12/25... Step: 3650... Loss: 1.3672... Val Loss: 1.5181\n",
            "Epoch: 12/25... Step: 3660... Loss: 1.3436... Val Loss: 1.5211\n",
            "Epoch: 12/25... Step: 3670... Loss: 1.3590... Val Loss: 1.5213\n",
            "Epoch: 12/25... Step: 3680... Loss: 1.3772... Val Loss: 1.5152\n",
            "Epoch: 12/25... Step: 3690... Loss: 1.3455... Val Loss: 1.5145\n",
            "Epoch: 12/25... Step: 3700... Loss: 1.3671... Val Loss: 1.5126\n",
            "Epoch: 12/25... Step: 3710... Loss: 1.3425... Val Loss: 1.5170\n",
            "Epoch: 12/25... Step: 3720... Loss: 1.3479... Val Loss: 1.5173\n",
            "Epoch: 12/25... Step: 3730... Loss: 1.3201... Val Loss: 1.5189\n",
            "Epoch: 12/25... Step: 3740... Loss: 1.3477... Val Loss: 1.5143\n",
            "Epoch: 12/25... Step: 3750... Loss: 1.3488... Val Loss: 1.5204\n",
            "Epoch: 12/25... Step: 3760... Loss: 1.3194... Val Loss: 1.5203\n",
            "Epoch: 12/25... Step: 3770... Loss: 1.3599... Val Loss: 1.5242\n",
            "Epoch: 12/25... Step: 3780... Loss: 1.3399... Val Loss: 1.5204\n",
            "Epoch: 12/25... Step: 3790... Loss: 1.3746... Val Loss: 1.5127\n",
            "Epoch: 12/25... Step: 3800... Loss: 1.3598... Val Loss: 1.5170\n",
            "Epoch: 12/25... Step: 3810... Loss: 1.3648... Val Loss: 1.5204\n",
            "Epoch: 12/25... Step: 3820... Loss: 1.3370... Val Loss: 1.5184\n",
            "Epoch: 12/25... Step: 3830... Loss: 1.3566... Val Loss: 1.5207\n",
            "Epoch: 12/25... Step: 3840... Loss: 1.3581... Val Loss: 1.5146\n",
            "Epoch: 12/25... Step: 3850... Loss: 1.3534... Val Loss: 1.5207\n",
            "Epoch: 13/25... Step: 3860... Loss: 1.3306... Val Loss: 1.5135\n",
            "Epoch: 13/25... Step: 3870... Loss: 1.3392... Val Loss: 1.5128\n",
            "Epoch: 13/25... Step: 3880... Loss: 1.3332... Val Loss: 1.5094\n",
            "Epoch: 13/25... Step: 3890... Loss: 1.3418... Val Loss: 1.5065\n",
            "Epoch: 13/25... Step: 3900... Loss: 1.3277... Val Loss: 1.5122\n",
            "Epoch: 13/25... Step: 3910... Loss: 1.3467... Val Loss: 1.5167\n",
            "Epoch: 13/25... Step: 3920... Loss: 1.3463... Val Loss: 1.5080\n",
            "Epoch: 13/25... Step: 3930... Loss: 1.3216... Val Loss: 1.5029\n",
            "Epoch: 13/25... Step: 3940... Loss: 1.3270... Val Loss: 1.5094\n",
            "Epoch: 13/25... Step: 3950... Loss: 1.3428... Val Loss: 1.5048\n",
            "Epoch: 13/25... Step: 3960... Loss: 1.3617... Val Loss: 1.5067\n",
            "Epoch: 13/25... Step: 3970... Loss: 1.3518... Val Loss: 1.5081\n",
            "Epoch: 13/25... Step: 3980... Loss: 1.3279... Val Loss: 1.5082\n",
            "Epoch: 13/25... Step: 3990... Loss: 1.3143... Val Loss: 1.5073\n",
            "Epoch: 13/25... Step: 4000... Loss: 1.3459... Val Loss: 1.5153\n",
            "Epoch: 13/25... Step: 4010... Loss: 1.3428... Val Loss: 1.5104\n",
            "Epoch: 13/25... Step: 4020... Loss: 1.3664... Val Loss: 1.5002\n",
            "Epoch: 13/25... Step: 4030... Loss: 1.3443... Val Loss: 1.5037\n",
            "Epoch: 13/25... Step: 4040... Loss: 1.3199... Val Loss: 1.5047\n",
            "Epoch: 13/25... Step: 4050... Loss: 1.3046... Val Loss: 1.5100\n",
            "Epoch: 13/25... Step: 4060... Loss: 1.3371... Val Loss: 1.5062\n",
            "Epoch: 13/25... Step: 4070... Loss: 1.3403... Val Loss: 1.5113\n",
            "Epoch: 13/25... Step: 4080... Loss: 1.3378... Val Loss: 1.5081\n",
            "Epoch: 13/25... Step: 4090... Loss: 1.3316... Val Loss: 1.5113\n",
            "Epoch: 13/25... Step: 4100... Loss: 1.3620... Val Loss: 1.5121\n",
            "Epoch: 13/25... Step: 4110... Loss: 1.3285... Val Loss: 1.5082\n",
            "Epoch: 13/25... Step: 4120... Loss: 1.3826... Val Loss: 1.5062\n",
            "Epoch: 13/25... Step: 4130... Loss: 1.3353... Val Loss: 1.5137\n",
            "Epoch: 13/25... Step: 4140... Loss: 1.3393... Val Loss: 1.5077\n",
            "Epoch: 13/25... Step: 4150... Loss: 1.3300... Val Loss: 1.5078\n",
            "Epoch: 13/25... Step: 4160... Loss: 1.3203... Val Loss: 1.5005\n",
            "Epoch: 13/25... Step: 4170... Loss: 1.3093... Val Loss: 1.5098\n",
            "Epoch: 14/25... Step: 4180... Loss: 1.3193... Val Loss: 1.5053\n",
            "Epoch: 14/25... Step: 4190... Loss: 1.3220... Val Loss: 1.5083\n",
            "Epoch: 14/25... Step: 4200... Loss: 1.3485... Val Loss: 1.5065\n",
            "Epoch: 14/25... Step: 4210... Loss: 1.3592... Val Loss: 1.5026\n",
            "Epoch: 14/25... Step: 4220... Loss: 1.3077... Val Loss: 1.4980\n",
            "Epoch: 14/25... Step: 4230... Loss: 1.3134... Val Loss: 1.5023\n",
            "Epoch: 14/25... Step: 4240... Loss: 1.3371... Val Loss: 1.5068\n",
            "Epoch: 14/25... Step: 4250... Loss: 1.3340... Val Loss: 1.4943\n",
            "Epoch: 14/25... Step: 4260... Loss: 1.3181... Val Loss: 1.4958\n",
            "Epoch: 14/25... Step: 4270... Loss: 1.3339... Val Loss: 1.4947\n",
            "Epoch: 14/25... Step: 4280... Loss: 1.3339... Val Loss: 1.5025\n",
            "Epoch: 14/25... Step: 4290... Loss: 1.3241... Val Loss: 1.5012\n",
            "Epoch: 14/25... Step: 4300... Loss: 1.3225... Val Loss: 1.4985\n",
            "Epoch: 14/25... Step: 4310... Loss: 1.3284... Val Loss: 1.4972\n",
            "Epoch: 14/25... Step: 4320... Loss: 1.3227... Val Loss: 1.5067\n",
            "Epoch: 14/25... Step: 4330... Loss: 1.3304... Val Loss: 1.5021\n",
            "Epoch: 14/25... Step: 4340... Loss: 1.3386... Val Loss: 1.4907\n",
            "Epoch: 14/25... Step: 4350... Loss: 1.3363... Val Loss: 1.4963\n",
            "Epoch: 14/25... Step: 4360... Loss: 1.3452... Val Loss: 1.4985\n",
            "Epoch: 14/25... Step: 4370... Loss: 1.3195... Val Loss: 1.4988\n",
            "Epoch: 14/25... Step: 4380... Loss: 1.3529... Val Loss: 1.5005\n",
            "Epoch: 14/25... Step: 4390... Loss: 1.3299... Val Loss: 1.5110\n",
            "Epoch: 14/25... Step: 4400... Loss: 1.3318... Val Loss: 1.4960\n",
            "Epoch: 14/25... Step: 4410... Loss: 1.3300... Val Loss: 1.5005\n",
            "Epoch: 14/25... Step: 4420... Loss: 1.3406... Val Loss: 1.5010\n",
            "Epoch: 14/25... Step: 4430... Loss: 1.3400... Val Loss: 1.4982\n",
            "Epoch: 14/25... Step: 4440... Loss: 1.3479... Val Loss: 1.5013\n",
            "Epoch: 14/25... Step: 4450... Loss: 1.3347... Val Loss: 1.4997\n",
            "Epoch: 14/25... Step: 4460... Loss: 1.3593... Val Loss: 1.4988\n",
            "Epoch: 14/25... Step: 4470... Loss: 1.3458... Val Loss: 1.5044\n",
            "Epoch: 14/25... Step: 4480... Loss: 1.3189... Val Loss: 1.4918\n",
            "Epoch: 14/25... Step: 4490... Loss: 1.3016... Val Loss: 1.5030\n",
            "Epoch: 15/25... Step: 4500... Loss: 1.3232... Val Loss: 1.4983\n",
            "Epoch: 15/25... Step: 4510... Loss: 1.3247... Val Loss: 1.5048\n",
            "Epoch: 15/25... Step: 4520... Loss: 1.3373... Val Loss: 1.4920\n",
            "Epoch: 15/25... Step: 4530... Loss: 1.3169... Val Loss: 1.4883\n",
            "Epoch: 15/25... Step: 4540... Loss: 1.3116... Val Loss: 1.4927\n",
            "Epoch: 15/25... Step: 4550... Loss: 1.3205... Val Loss: 1.4900\n",
            "Epoch: 15/25... Step: 4560... Loss: 1.3210... Val Loss: 1.4911\n",
            "Epoch: 15/25... Step: 4570... Loss: 1.3158... Val Loss: 1.4896\n",
            "Epoch: 15/25... Step: 4580... Loss: 1.3294... Val Loss: 1.4879\n",
            "Epoch: 15/25... Step: 4590... Loss: 1.3300... Val Loss: 1.4841\n",
            "Epoch: 15/25... Step: 4600... Loss: 1.3225... Val Loss: 1.4970\n",
            "Epoch: 15/25... Step: 4610... Loss: 1.3527... Val Loss: 1.4958\n",
            "Epoch: 15/25... Step: 4620... Loss: 1.3487... Val Loss: 1.4895\n",
            "Epoch: 15/25... Step: 4630... Loss: 1.3062... Val Loss: 1.4866\n",
            "Epoch: 15/25... Step: 4640... Loss: 1.3116... Val Loss: 1.4906\n",
            "Epoch: 15/25... Step: 4650... Loss: 1.3033... Val Loss: 1.4938\n",
            "Epoch: 15/25... Step: 4660... Loss: 1.3476... Val Loss: 1.4854\n",
            "Epoch: 15/25... Step: 4670... Loss: 1.3176... Val Loss: 1.4941\n",
            "Epoch: 15/25... Step: 4680... Loss: 1.3320... Val Loss: 1.4911\n",
            "Epoch: 15/25... Step: 4690... Loss: 1.2946... Val Loss: 1.4885\n",
            "Epoch: 15/25... Step: 4700... Loss: 1.3016... Val Loss: 1.4915\n",
            "Epoch: 15/25... Step: 4710... Loss: 1.3251... Val Loss: 1.4921\n",
            "Epoch: 15/25... Step: 4720... Loss: 1.3082... Val Loss: 1.4899\n",
            "Epoch: 15/25... Step: 4730... Loss: 1.3280... Val Loss: 1.4940\n",
            "Epoch: 15/25... Step: 4740... Loss: 1.3479... Val Loss: 1.4995\n",
            "Epoch: 15/25... Step: 4750... Loss: 1.3325... Val Loss: 1.4943\n",
            "Epoch: 15/25... Step: 4760... Loss: 1.3273... Val Loss: 1.5021\n",
            "Epoch: 15/25... Step: 4770... Loss: 1.3143... Val Loss: 1.4972\n",
            "Epoch: 15/25... Step: 4780... Loss: 1.3272... Val Loss: 1.5013\n",
            "Epoch: 15/25... Step: 4790... Loss: 1.3302... Val Loss: 1.5003\n",
            "Epoch: 15/25... Step: 4800... Loss: 1.3199... Val Loss: 1.4881\n",
            "Epoch: 15/25... Step: 4810... Loss: 1.3266... Val Loss: 1.4920\n",
            "Epoch: 16/25... Step: 4820... Loss: 1.3176... Val Loss: 1.4906\n",
            "Epoch: 16/25... Step: 4830... Loss: 1.3260... Val Loss: 1.4986\n",
            "Epoch: 16/25... Step: 4840... Loss: 1.3274... Val Loss: 1.4870\n",
            "Epoch: 16/25... Step: 4850... Loss: 1.3430... Val Loss: 1.4886\n",
            "Epoch: 16/25... Step: 4860... Loss: 1.3024... Val Loss: 1.4842\n",
            "Epoch: 16/25... Step: 4870... Loss: 1.3011... Val Loss: 1.4839\n",
            "Epoch: 16/25... Step: 4880... Loss: 1.3120... Val Loss: 1.4814\n",
            "Epoch: 16/25... Step: 4890... Loss: 1.3066... Val Loss: 1.4854\n",
            "Epoch: 16/25... Step: 4900... Loss: 1.3005... Val Loss: 1.4824\n",
            "Epoch: 16/25... Step: 4910... Loss: 1.3288... Val Loss: 1.4819\n",
            "Epoch: 16/25... Step: 4920... Loss: 1.3081... Val Loss: 1.4879\n",
            "Epoch: 16/25... Step: 4930... Loss: 1.3377... Val Loss: 1.4831\n",
            "Epoch: 16/25... Step: 4940... Loss: 1.3220... Val Loss: 1.4842\n",
            "Epoch: 16/25... Step: 4950... Loss: 1.3120... Val Loss: 1.4888\n",
            "Epoch: 16/25... Step: 4960... Loss: 1.3326... Val Loss: 1.4920\n",
            "Epoch: 16/25... Step: 4970... Loss: 1.2954... Val Loss: 1.4967\n",
            "Epoch: 16/25... Step: 4980... Loss: 1.3539... Val Loss: 1.4869\n",
            "Epoch: 16/25... Step: 4990... Loss: 1.3098... Val Loss: 1.4901\n",
            "Epoch: 16/25... Step: 5000... Loss: 1.3239... Val Loss: 1.4899\n",
            "Epoch: 16/25... Step: 5010... Loss: 1.3242... Val Loss: 1.4894\n",
            "Epoch: 16/25... Step: 5020... Loss: 1.3100... Val Loss: 1.4901\n",
            "Epoch: 16/25... Step: 5030... Loss: 1.3020... Val Loss: 1.4954\n",
            "Epoch: 16/25... Step: 5040... Loss: 1.3234... Val Loss: 1.4878\n",
            "Epoch: 16/25... Step: 5050... Loss: 1.3370... Val Loss: 1.4907\n",
            "Epoch: 16/25... Step: 5060... Loss: 1.3324... Val Loss: 1.4907\n",
            "Epoch: 16/25... Step: 5070... Loss: 1.2986... Val Loss: 1.4870\n",
            "Epoch: 16/25... Step: 5080... Loss: 1.3000... Val Loss: 1.4917\n",
            "Epoch: 16/25... Step: 5090... Loss: 1.3120... Val Loss: 1.4922\n",
            "Epoch: 16/25... Step: 5100... Loss: 1.2838... Val Loss: 1.4926\n",
            "Epoch: 16/25... Step: 5110... Loss: 1.3125... Val Loss: 1.5006\n",
            "Epoch: 16/25... Step: 5120... Loss: 1.3201... Val Loss: 1.4922\n",
            "Epoch: 16/25... Step: 5130... Loss: 1.2979... Val Loss: 1.4829\n",
            "Epoch: 17/25... Step: 5140... Loss: 1.3211... Val Loss: 1.4817\n",
            "Epoch: 17/25... Step: 5150... Loss: 1.2963... Val Loss: 1.4908\n",
            "Epoch: 17/25... Step: 5160... Loss: 1.3178... Val Loss: 1.4810\n",
            "Epoch: 17/25... Step: 5170... Loss: 1.3102... Val Loss: 1.4835\n",
            "Epoch: 17/25... Step: 5180... Loss: 1.3010... Val Loss: 1.4787\n",
            "Epoch: 17/25... Step: 5190... Loss: 1.2648... Val Loss: 1.4823\n",
            "Epoch: 17/25... Step: 5200... Loss: 1.2876... Val Loss: 1.4773\n",
            "Epoch: 17/25... Step: 5210... Loss: 1.2999... Val Loss: 1.4863\n",
            "Epoch: 17/25... Step: 5220... Loss: 1.2686... Val Loss: 1.4812\n",
            "Epoch: 17/25... Step: 5230... Loss: 1.3191... Val Loss: 1.4803\n",
            "Epoch: 17/25... Step: 5240... Loss: 1.2953... Val Loss: 1.4801\n",
            "Epoch: 17/25... Step: 5250... Loss: 1.3299... Val Loss: 1.4792\n",
            "Epoch: 17/25... Step: 5260... Loss: 1.3251... Val Loss: 1.4815\n",
            "Epoch: 17/25... Step: 5270... Loss: 1.2891... Val Loss: 1.4747\n",
            "Epoch: 17/25... Step: 5280... Loss: 1.3190... Val Loss: 1.4800\n",
            "Epoch: 17/25... Step: 5290... Loss: 1.2850... Val Loss: 1.4824\n",
            "Epoch: 17/25... Step: 5300... Loss: 1.2999... Val Loss: 1.4794\n",
            "Epoch: 17/25... Step: 5310... Loss: 1.2932... Val Loss: 1.4822\n",
            "Epoch: 17/25... Step: 5320... Loss: 1.2796... Val Loss: 1.4778\n",
            "Epoch: 17/25... Step: 5330... Loss: 1.3054... Val Loss: 1.4770\n",
            "Epoch: 17/25... Step: 5340... Loss: 1.2861... Val Loss: 1.4825\n",
            "Epoch: 17/25... Step: 5350... Loss: 1.2884... Val Loss: 1.4834\n",
            "Epoch: 17/25... Step: 5360... Loss: 1.3063... Val Loss: 1.4827\n",
            "Epoch: 17/25... Step: 5370... Loss: 1.3156... Val Loss: 1.4808\n",
            "Epoch: 17/25... Step: 5380... Loss: 1.2821... Val Loss: 1.4862\n",
            "Epoch: 17/25... Step: 5390... Loss: 1.2998... Val Loss: 1.4806\n",
            "Epoch: 17/25... Step: 5400... Loss: 1.3028... Val Loss: 1.4842\n",
            "Epoch: 17/25... Step: 5410... Loss: 1.3088... Val Loss: 1.4827\n",
            "Epoch: 17/25... Step: 5420... Loss: 1.2970... Val Loss: 1.4831\n",
            "Epoch: 17/25... Step: 5430... Loss: 1.3036... Val Loss: 1.4859\n",
            "Epoch: 17/25... Step: 5440... Loss: 1.2827... Val Loss: 1.4853\n",
            "Epoch: 17/25... Step: 5450... Loss: 1.3142... Val Loss: 1.4806\n",
            "Epoch: 18/25... Step: 5460... Loss: 1.2828... Val Loss: 1.4804\n",
            "Epoch: 18/25... Step: 5470... Loss: 1.2827... Val Loss: 1.4905\n",
            "Epoch: 18/25... Step: 5480... Loss: 1.3094... Val Loss: 1.4840\n",
            "Epoch: 18/25... Step: 5490... Loss: 1.2736... Val Loss: 1.4786\n",
            "Epoch: 18/25... Step: 5500... Loss: 1.2677... Val Loss: 1.4768\n",
            "Epoch: 18/25... Step: 5510... Loss: 1.2545... Val Loss: 1.4738\n",
            "Epoch: 18/25... Step: 5520... Loss: 1.3004... Val Loss: 1.4733\n",
            "Epoch: 18/25... Step: 5530... Loss: 1.3119... Val Loss: 1.4713\n",
            "Epoch: 18/25... Step: 5540... Loss: 1.2983... Val Loss: 1.4764\n",
            "Epoch: 18/25... Step: 5550... Loss: 1.2769... Val Loss: 1.4746\n",
            "Epoch: 18/25... Step: 5560... Loss: 1.2940... Val Loss: 1.4792\n",
            "Epoch: 18/25... Step: 5570... Loss: 1.2922... Val Loss: 1.4714\n",
            "Epoch: 18/25... Step: 5580... Loss: 1.3197... Val Loss: 1.4799\n",
            "Epoch: 18/25... Step: 5590... Loss: 1.2915... Val Loss: 1.4709\n",
            "Epoch: 18/25... Step: 5600... Loss: 1.3174... Val Loss: 1.4734\n",
            "Epoch: 18/25... Step: 5610... Loss: 1.2899... Val Loss: 1.4792\n",
            "Epoch: 18/25... Step: 5620... Loss: 1.3112... Val Loss: 1.4726\n",
            "Epoch: 18/25... Step: 5630... Loss: 1.2644... Val Loss: 1.4742\n",
            "Epoch: 18/25... Step: 5640... Loss: 1.2891... Val Loss: 1.4775\n",
            "Epoch: 18/25... Step: 5650... Loss: 1.3034... Val Loss: 1.4722\n",
            "Epoch: 18/25... Step: 5660... Loss: 1.2915... Val Loss: 1.4770\n",
            "Epoch: 18/25... Step: 5670... Loss: 1.2944... Val Loss: 1.4745\n",
            "Epoch: 18/25... Step: 5680... Loss: 1.3187... Val Loss: 1.4779\n",
            "Epoch: 18/25... Step: 5690... Loss: 1.2886... Val Loss: 1.4800\n",
            "Epoch: 18/25... Step: 5700... Loss: 1.3068... Val Loss: 1.4833\n",
            "Epoch: 18/25... Step: 5710... Loss: 1.2791... Val Loss: 1.4793\n",
            "Epoch: 18/25... Step: 5720... Loss: 1.2895... Val Loss: 1.4834\n",
            "Epoch: 18/25... Step: 5730... Loss: 1.2777... Val Loss: 1.4819\n",
            "Epoch: 18/25... Step: 5740... Loss: 1.2837... Val Loss: 1.4805\n",
            "Epoch: 18/25... Step: 5750... Loss: 1.3002... Val Loss: 1.4819\n",
            "Epoch: 18/25... Step: 5760... Loss: 1.3045... Val Loss: 1.4751\n",
            "Epoch: 18/25... Step: 5770... Loss: 1.2839... Val Loss: 1.4762\n",
            "Epoch: 19/25... Step: 5780... Loss: 1.2708... Val Loss: 1.4796\n",
            "Epoch: 19/25... Step: 5790... Loss: 1.2710... Val Loss: 1.4833\n",
            "Epoch: 19/25... Step: 5800... Loss: 1.2862... Val Loss: 1.4770\n",
            "Epoch: 19/25... Step: 5810... Loss: 1.2642... Val Loss: 1.4819\n",
            "Epoch: 19/25... Step: 5820... Loss: 1.2785... Val Loss: 1.4728\n",
            "Epoch: 19/25... Step: 5830... Loss: 1.3057... Val Loss: 1.4682\n",
            "Epoch: 19/25... Step: 5840... Loss: 1.2881... Val Loss: 1.4705\n",
            "Epoch: 19/25... Step: 5850... Loss: 1.2731... Val Loss: 1.4671\n",
            "Epoch: 19/25... Step: 5860... Loss: 1.2813... Val Loss: 1.4710\n",
            "Epoch: 19/25... Step: 5870... Loss: 1.2495... Val Loss: 1.4702\n",
            "Epoch: 19/25... Step: 5880... Loss: 1.2773... Val Loss: 1.4717\n",
            "Epoch: 19/25... Step: 5890... Loss: 1.2868... Val Loss: 1.4727\n",
            "Epoch: 19/25... Step: 5900... Loss: 1.3096... Val Loss: 1.4774\n",
            "Epoch: 19/25... Step: 5910... Loss: 1.2653... Val Loss: 1.4688\n",
            "Epoch: 19/25... Step: 5920... Loss: 1.3121... Val Loss: 1.4696\n",
            "Epoch: 19/25... Step: 5930... Loss: 1.2811... Val Loss: 1.4718\n",
            "Epoch: 19/25... Step: 5940... Loss: 1.3141... Val Loss: 1.4677\n",
            "Epoch: 19/25... Step: 5950... Loss: 1.2764... Val Loss: 1.4710\n",
            "Epoch: 19/25... Step: 5960... Loss: 1.2830... Val Loss: 1.4728\n",
            "Epoch: 19/25... Step: 5970... Loss: 1.2986... Val Loss: 1.4630\n",
            "Epoch: 19/25... Step: 5980... Loss: 1.2910... Val Loss: 1.4721\n",
            "Epoch: 19/25... Step: 5990... Loss: 1.2841... Val Loss: 1.4728\n",
            "Epoch: 19/25... Step: 6000... Loss: 1.2897... Val Loss: 1.4760\n",
            "Epoch: 19/25... Step: 6010... Loss: 1.3069... Val Loss: 1.4758\n",
            "Epoch: 19/25... Step: 6020... Loss: 1.3143... Val Loss: 1.4769\n",
            "Epoch: 19/25... Step: 6030... Loss: 1.3016... Val Loss: 1.4790\n",
            "Epoch: 19/25... Step: 6040... Loss: 1.2934... Val Loss: 1.4770\n",
            "Epoch: 19/25... Step: 6050... Loss: 1.2832... Val Loss: 1.4752\n",
            "Epoch: 19/25... Step: 6060... Loss: 1.2898... Val Loss: 1.4805\n",
            "Epoch: 19/25... Step: 6070... Loss: 1.2701... Val Loss: 1.4748\n",
            "Epoch: 19/25... Step: 6080... Loss: 1.2569... Val Loss: 1.4796\n",
            "Epoch: 19/25... Step: 6090... Loss: 1.2896... Val Loss: 1.4717\n",
            "Epoch: 20/25... Step: 6100... Loss: 1.3641... Val Loss: 1.4738\n",
            "Epoch: 20/25... Step: 6110... Loss: 1.2701... Val Loss: 1.4739\n",
            "Epoch: 20/25... Step: 6120... Loss: 1.2821... Val Loss: 1.4776\n",
            "Epoch: 20/25... Step: 6130... Loss: 1.2733... Val Loss: 1.4776\n",
            "Epoch: 20/25... Step: 6140... Loss: 1.2698... Val Loss: 1.4707\n",
            "Epoch: 20/25... Step: 6150... Loss: 1.2746... Val Loss: 1.4670\n",
            "Epoch: 20/25... Step: 6160... Loss: 1.2676... Val Loss: 1.4689\n",
            "Epoch: 20/25... Step: 6170... Loss: 1.2900... Val Loss: 1.4736\n",
            "Epoch: 20/25... Step: 6180... Loss: 1.2865... Val Loss: 1.4706\n",
            "Epoch: 20/25... Step: 6190... Loss: 1.2847... Val Loss: 1.4694\n",
            "Epoch: 20/25... Step: 6200... Loss: 1.2999... Val Loss: 1.4765\n",
            "Epoch: 20/25... Step: 6210... Loss: 1.2713... Val Loss: 1.4761\n",
            "Epoch: 20/25... Step: 6220... Loss: 1.2753... Val Loss: 1.4788\n",
            "Epoch: 20/25... Step: 6230... Loss: 1.2782... Val Loss: 1.4721\n",
            "Epoch: 20/25... Step: 6240... Loss: 1.2945... Val Loss: 1.4714\n",
            "Epoch: 20/25... Step: 6250... Loss: 1.2906... Val Loss: 1.4719\n",
            "Epoch: 20/25... Step: 6260... Loss: 1.2802... Val Loss: 1.4715\n",
            "Epoch: 20/25... Step: 6270... Loss: 1.2985... Val Loss: 1.4707\n",
            "Epoch: 20/25... Step: 6280... Loss: 1.2830... Val Loss: 1.4743\n",
            "Epoch: 20/25... Step: 6290... Loss: 1.3106... Val Loss: 1.4705\n",
            "Epoch: 20/25... Step: 6300... Loss: 1.2518... Val Loss: 1.4745\n",
            "Epoch: 20/25... Step: 6310... Loss: 1.2915... Val Loss: 1.4715\n",
            "Epoch: 20/25... Step: 6320... Loss: 1.3055... Val Loss: 1.4731\n",
            "Epoch: 20/25... Step: 6330... Loss: 1.2910... Val Loss: 1.4715\n",
            "Epoch: 20/25... Step: 6340... Loss: 1.3002... Val Loss: 1.4691\n",
            "Epoch: 20/25... Step: 6350... Loss: 1.2690... Val Loss: 1.4747\n",
            "Epoch: 20/25... Step: 6360... Loss: 1.2781... Val Loss: 1.4715\n",
            "Epoch: 20/25... Step: 6370... Loss: 1.2673... Val Loss: 1.4717\n",
            "Epoch: 20/25... Step: 6380... Loss: 1.2785... Val Loss: 1.4743\n",
            "Epoch: 20/25... Step: 6390... Loss: 1.2718... Val Loss: 1.4707\n",
            "Epoch: 20/25... Step: 6400... Loss: 1.2460... Val Loss: 1.4723\n",
            "Epoch: 20/25... Step: 6410... Loss: 1.2874... Val Loss: 1.4685\n",
            "Epoch: 20/25... Step: 6420... Loss: 1.3752... Val Loss: 1.4711\n",
            "Epoch: 21/25... Step: 6430... Loss: 1.2670... Val Loss: 1.4725\n",
            "Epoch: 21/25... Step: 6440... Loss: 1.3034... Val Loss: 1.4690\n",
            "Epoch: 21/25... Step: 6450... Loss: 1.2517... Val Loss: 1.4748\n",
            "Epoch: 21/25... Step: 6460... Loss: 1.2597... Val Loss: 1.4650\n",
            "Epoch: 21/25... Step: 6470... Loss: 1.2668... Val Loss: 1.4658\n",
            "Epoch: 21/25... Step: 6480... Loss: 1.2609... Val Loss: 1.4660\n",
            "Epoch: 21/25... Step: 6490... Loss: 1.2736... Val Loss: 1.4652\n",
            "Epoch: 21/25... Step: 6500... Loss: 1.2857... Val Loss: 1.4684\n",
            "Epoch: 21/25... Step: 6510... Loss: 1.2597... Val Loss: 1.4682\n",
            "Epoch: 21/25... Step: 6520... Loss: 1.2903... Val Loss: 1.4688\n",
            "Epoch: 21/25... Step: 6530... Loss: 1.3193... Val Loss: 1.4694\n",
            "Epoch: 21/25... Step: 6540... Loss: 1.2686... Val Loss: 1.4711\n",
            "Epoch: 21/25... Step: 6550... Loss: 1.2569... Val Loss: 1.4719\n",
            "Epoch: 21/25... Step: 6560... Loss: 1.2799... Val Loss: 1.4664\n",
            "Epoch: 21/25... Step: 6570... Loss: 1.2845... Val Loss: 1.4666\n",
            "Epoch: 21/25... Step: 6580... Loss: 1.2681... Val Loss: 1.4724\n",
            "Epoch: 21/25... Step: 6590... Loss: 1.2539... Val Loss: 1.4691\n",
            "Epoch: 21/25... Step: 6600... Loss: 1.2608... Val Loss: 1.4681\n",
            "Epoch: 21/25... Step: 6610... Loss: 1.2632... Val Loss: 1.4647\n",
            "Epoch: 21/25... Step: 6620... Loss: 1.2272... Val Loss: 1.4717\n",
            "Epoch: 21/25... Step: 6630... Loss: 1.2984... Val Loss: 1.4657\n",
            "Epoch: 21/25... Step: 6640... Loss: 1.2727... Val Loss: 1.4676\n",
            "Epoch: 21/25... Step: 6650... Loss: 1.2776... Val Loss: 1.4662\n",
            "Epoch: 21/25... Step: 6660... Loss: 1.2684... Val Loss: 1.4667\n",
            "Epoch: 21/25... Step: 6670... Loss: 1.2515... Val Loss: 1.4712\n",
            "Epoch: 21/25... Step: 6680... Loss: 1.2910... Val Loss: 1.4711\n",
            "Epoch: 21/25... Step: 6690... Loss: 1.2920... Val Loss: 1.4714\n",
            "Epoch: 21/25... Step: 6700... Loss: 1.2748... Val Loss: 1.4705\n",
            "Epoch: 21/25... Step: 6710... Loss: 1.2633... Val Loss: 1.4702\n",
            "Epoch: 21/25... Step: 6720... Loss: 1.2764... Val Loss: 1.4716\n",
            "Epoch: 21/25... Step: 6730... Loss: 1.2511... Val Loss: 1.4682\n",
            "Epoch: 21/25... Step: 6740... Loss: 1.2603... Val Loss: 1.4689\n",
            "Epoch: 22/25... Step: 6750... Loss: 1.2753... Val Loss: 1.4659\n",
            "Epoch: 22/25... Step: 6760... Loss: 1.2667... Val Loss: 1.4674\n",
            "Epoch: 22/25... Step: 6770... Loss: 1.2674... Val Loss: 1.4756\n",
            "Epoch: 22/25... Step: 6780... Loss: 1.2811... Val Loss: 1.4641\n",
            "Epoch: 22/25... Step: 6790... Loss: 1.2349... Val Loss: 1.4645\n",
            "Epoch: 22/25... Step: 6800... Loss: 1.2511... Val Loss: 1.4663\n",
            "Epoch: 22/25... Step: 6810... Loss: 1.2598... Val Loss: 1.4575\n",
            "Epoch: 22/25... Step: 6820... Loss: 1.2752... Val Loss: 1.4661\n",
            "Epoch: 22/25... Step: 6830... Loss: 1.2386... Val Loss: 1.4669\n",
            "Epoch: 22/25... Step: 6840... Loss: 1.2860... Val Loss: 1.4695\n",
            "Epoch: 22/25... Step: 6850... Loss: 1.3049... Val Loss: 1.4663\n",
            "Epoch: 22/25... Step: 6860... Loss: 1.2760... Val Loss: 1.4742\n",
            "Epoch: 22/25... Step: 6870... Loss: 1.2541... Val Loss: 1.4709\n",
            "Epoch: 22/25... Step: 6880... Loss: 1.2696... Val Loss: 1.4629\n",
            "Epoch: 22/25... Step: 6890... Loss: 1.2781... Val Loss: 1.4664\n",
            "Epoch: 22/25... Step: 6900... Loss: 1.2512... Val Loss: 1.4707\n",
            "Epoch: 22/25... Step: 6910... Loss: 1.2687... Val Loss: 1.4627\n",
            "Epoch: 22/25... Step: 6920... Loss: 1.2475... Val Loss: 1.4645\n",
            "Epoch: 22/25... Step: 6930... Loss: 1.2603... Val Loss: 1.4633\n",
            "Epoch: 22/25... Step: 6940... Loss: 1.2345... Val Loss: 1.4626\n",
            "Epoch: 22/25... Step: 6950... Loss: 1.2481... Val Loss: 1.4594\n",
            "Epoch: 22/25... Step: 6960... Loss: 1.2552... Val Loss: 1.4684\n",
            "Epoch: 22/25... Step: 6970... Loss: 1.2435... Val Loss: 1.4657\n",
            "Epoch: 22/25... Step: 6980... Loss: 1.2761... Val Loss: 1.4651\n",
            "Epoch: 22/25... Step: 6990... Loss: 1.2513... Val Loss: 1.4679\n",
            "Epoch: 22/25... Step: 7000... Loss: 1.2843... Val Loss: 1.4639\n",
            "Epoch: 22/25... Step: 7010... Loss: 1.2709... Val Loss: 1.4688\n",
            "Epoch: 22/25... Step: 7020... Loss: 1.2730... Val Loss: 1.4689\n",
            "Epoch: 22/25... Step: 7030... Loss: 1.2557... Val Loss: 1.4663\n",
            "Epoch: 22/25... Step: 7040... Loss: 1.2667... Val Loss: 1.4661\n",
            "Epoch: 22/25... Step: 7050... Loss: 1.2668... Val Loss: 1.4636\n",
            "Epoch: 22/25... Step: 7060... Loss: 1.2624... Val Loss: 1.4659\n",
            "Epoch: 23/25... Step: 7070... Loss: 1.2464... Val Loss: 1.4635\n",
            "Epoch: 23/25... Step: 7080... Loss: 1.2494... Val Loss: 1.4627\n",
            "Epoch: 23/25... Step: 7090... Loss: 1.2401... Val Loss: 1.4674\n",
            "Epoch: 23/25... Step: 7100... Loss: 1.2532... Val Loss: 1.4611\n",
            "Epoch: 23/25... Step: 7110... Loss: 1.2411... Val Loss: 1.4572\n",
            "Epoch: 23/25... Step: 7120... Loss: 1.2595... Val Loss: 1.4557\n",
            "Epoch: 23/25... Step: 7130... Loss: 1.2615... Val Loss: 1.4544\n",
            "Epoch: 23/25... Step: 7140... Loss: 1.2502... Val Loss: 1.4570\n",
            "Epoch: 23/25... Step: 7150... Loss: 1.2473... Val Loss: 1.4554\n",
            "Epoch: 23/25... Step: 7160... Loss: 1.2520... Val Loss: 1.4609\n",
            "Epoch: 23/25... Step: 7170... Loss: 1.2706... Val Loss: 1.4592\n",
            "Epoch: 23/25... Step: 7180... Loss: 1.2633... Val Loss: 1.4605\n",
            "Epoch: 23/25... Step: 7190... Loss: 1.2469... Val Loss: 1.4593\n",
            "Epoch: 23/25... Step: 7200... Loss: 1.2377... Val Loss: 1.4577\n",
            "Epoch: 23/25... Step: 7210... Loss: 1.2654... Val Loss: 1.4591\n",
            "Epoch: 23/25... Step: 7220... Loss: 1.2457... Val Loss: 1.4614\n",
            "Epoch: 23/25... Step: 7230... Loss: 1.2956... Val Loss: 1.4576\n",
            "Epoch: 23/25... Step: 7240... Loss: 1.2618... Val Loss: 1.4640\n",
            "Epoch: 23/25... Step: 7250... Loss: 1.2430... Val Loss: 1.4634\n",
            "Epoch: 23/25... Step: 7260... Loss: 1.2239... Val Loss: 1.4596\n",
            "Epoch: 23/25... Step: 7270... Loss: 1.2660... Val Loss: 1.4619\n",
            "Epoch: 23/25... Step: 7280... Loss: 1.2536... Val Loss: 1.4677\n",
            "Epoch: 23/25... Step: 7290... Loss: 1.2606... Val Loss: 1.4598\n",
            "Epoch: 23/25... Step: 7300... Loss: 1.2512... Val Loss: 1.4645\n",
            "Epoch: 23/25... Step: 7310... Loss: 1.2734... Val Loss: 1.4674\n",
            "Epoch: 23/25... Step: 7320... Loss: 1.2586... Val Loss: 1.4665\n",
            "Epoch: 23/25... Step: 7330... Loss: 1.2900... Val Loss: 1.4668\n",
            "Epoch: 23/25... Step: 7340... Loss: 1.2566... Val Loss: 1.4643\n",
            "Epoch: 23/25... Step: 7350... Loss: 1.2557... Val Loss: 1.4680\n",
            "Epoch: 23/25... Step: 7360... Loss: 1.2588... Val Loss: 1.4695\n",
            "Epoch: 23/25... Step: 7370... Loss: 1.2489... Val Loss: 1.4597\n",
            "Epoch: 23/25... Step: 7380... Loss: 1.2356... Val Loss: 1.4661\n",
            "Epoch: 24/25... Step: 7390... Loss: 1.2506... Val Loss: 1.4586\n",
            "Epoch: 24/25... Step: 7400... Loss: 1.2458... Val Loss: 1.4631\n",
            "Epoch: 24/25... Step: 7410... Loss: 1.2688... Val Loss: 1.4627\n",
            "Epoch: 24/25... Step: 7420... Loss: 1.2778... Val Loss: 1.4578\n",
            "Epoch: 24/25... Step: 7430... Loss: 1.2385... Val Loss: 1.4594\n",
            "Epoch: 24/25... Step: 7440... Loss: 1.2461... Val Loss: 1.4564\n",
            "Epoch: 24/25... Step: 7450... Loss: 1.2644... Val Loss: 1.4616\n",
            "Epoch: 24/25... Step: 7460... Loss: 1.2421... Val Loss: 1.4665\n",
            "Epoch: 24/25... Step: 7470... Loss: 1.2502... Val Loss: 1.4561\n",
            "Epoch: 24/25... Step: 7480... Loss: 1.2482... Val Loss: 1.4559\n",
            "Epoch: 24/25... Step: 7490... Loss: 1.2535... Val Loss: 1.4635\n",
            "Epoch: 24/25... Step: 7500... Loss: 1.2510... Val Loss: 1.4562\n",
            "Epoch: 24/25... Step: 7510... Loss: 1.2512... Val Loss: 1.4576\n",
            "Epoch: 24/25... Step: 7520... Loss: 1.2516... Val Loss: 1.4546\n",
            "Epoch: 24/25... Step: 7530... Loss: 1.2439... Val Loss: 1.4565\n",
            "Epoch: 24/25... Step: 7540... Loss: 1.2558... Val Loss: 1.4574\n",
            "Epoch: 24/25... Step: 7550... Loss: 1.2571... Val Loss: 1.4559\n",
            "Epoch: 24/25... Step: 7560... Loss: 1.2464... Val Loss: 1.4587\n",
            "Epoch: 24/25... Step: 7570... Loss: 1.2651... Val Loss: 1.4582\n",
            "Epoch: 24/25... Step: 7580... Loss: 1.2382... Val Loss: 1.4553\n",
            "Epoch: 24/25... Step: 7590... Loss: 1.2715... Val Loss: 1.4602\n",
            "Epoch: 24/25... Step: 7600... Loss: 1.2564... Val Loss: 1.4643\n",
            "Epoch: 24/25... Step: 7610... Loss: 1.2571... Val Loss: 1.4609\n",
            "Epoch: 24/25... Step: 7620... Loss: 1.2594... Val Loss: 1.4606\n",
            "Epoch: 24/25... Step: 7630... Loss: 1.2628... Val Loss: 1.4635\n",
            "Epoch: 24/25... Step: 7640... Loss: 1.2594... Val Loss: 1.4608\n",
            "Epoch: 24/25... Step: 7650... Loss: 1.2830... Val Loss: 1.4678\n",
            "Epoch: 24/25... Step: 7660... Loss: 1.2610... Val Loss: 1.4703\n",
            "Epoch: 24/25... Step: 7670... Loss: 1.2905... Val Loss: 1.4631\n",
            "Epoch: 24/25... Step: 7680... Loss: 1.2684... Val Loss: 1.4659\n",
            "Epoch: 24/25... Step: 7690... Loss: 1.2545... Val Loss: 1.4611\n",
            "Epoch: 24/25... Step: 7700... Loss: 1.2396... Val Loss: 1.4619\n",
            "Epoch: 25/25... Step: 7710... Loss: 1.2519... Val Loss: 1.4520\n",
            "Epoch: 25/25... Step: 7720... Loss: 1.2528... Val Loss: 1.4593\n",
            "Epoch: 25/25... Step: 7730... Loss: 1.2607... Val Loss: 1.4588\n",
            "Epoch: 25/25... Step: 7740... Loss: 1.2444... Val Loss: 1.4636\n",
            "Epoch: 25/25... Step: 7750... Loss: 1.2384... Val Loss: 1.4558\n",
            "Epoch: 25/25... Step: 7760... Loss: 1.2491... Val Loss: 1.4556\n",
            "Epoch: 25/25... Step: 7770... Loss: 1.2542... Val Loss: 1.4562\n",
            "Epoch: 25/25... Step: 7780... Loss: 1.2492... Val Loss: 1.4606\n",
            "Epoch: 25/25... Step: 7790... Loss: 1.2650... Val Loss: 1.4576\n",
            "Epoch: 25/25... Step: 7800... Loss: 1.2632... Val Loss: 1.4561\n",
            "Epoch: 25/25... Step: 7810... Loss: 1.2479... Val Loss: 1.4577\n",
            "Epoch: 25/25... Step: 7820... Loss: 1.2805... Val Loss: 1.4577\n",
            "Epoch: 25/25... Step: 7830... Loss: 1.2742... Val Loss: 1.4592\n",
            "Epoch: 25/25... Step: 7840... Loss: 1.2379... Val Loss: 1.4591\n",
            "Epoch: 25/25... Step: 7850... Loss: 1.2438... Val Loss: 1.4539\n",
            "Epoch: 25/25... Step: 7860... Loss: 1.2428... Val Loss: 1.4582\n",
            "Epoch: 25/25... Step: 7870... Loss: 1.2894... Val Loss: 1.4539\n",
            "Epoch: 25/25... Step: 7880... Loss: 1.2408... Val Loss: 1.4541\n",
            "Epoch: 25/25... Step: 7890... Loss: 1.2732... Val Loss: 1.4532\n",
            "Epoch: 25/25... Step: 7900... Loss: 1.2213... Val Loss: 1.4521\n",
            "Epoch: 25/25... Step: 7910... Loss: 1.2394... Val Loss: 1.4561\n",
            "Epoch: 25/25... Step: 7920... Loss: 1.2571... Val Loss: 1.4588\n",
            "Epoch: 25/25... Step: 7930... Loss: 1.2312... Val Loss: 1.4549\n",
            "Epoch: 25/25... Step: 7940... Loss: 1.2573... Val Loss: 1.4566\n",
            "Epoch: 25/25... Step: 7950... Loss: 1.2794... Val Loss: 1.4625\n",
            "Epoch: 25/25... Step: 7960... Loss: 1.2646... Val Loss: 1.4590\n",
            "Epoch: 25/25... Step: 7970... Loss: 1.2547... Val Loss: 1.4631\n",
            "Epoch: 25/25... Step: 7980... Loss: 1.2381... Val Loss: 1.4608\n",
            "Epoch: 25/25... Step: 7990... Loss: 1.2668... Val Loss: 1.4633\n",
            "Epoch: 25/25... Step: 8000... Loss: 1.2579... Val Loss: 1.4655\n",
            "Epoch: 25/25... Step: 8010... Loss: 1.2479... Val Loss: 1.4595\n",
            "Epoch: 25/25... Step: 8020... Loss: 1.2517... Val Loss: 1.4560\n"
          ]
        }
      ],
      "source": [
        "train(\n",
        "    net,\n",
        "    encoded_text,\n",
        "    epochs=25,\n",
        "    n_seqs=128,\n",
        "    n_steps=100,\n",
        "    lr=0.001,\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    print_every=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGi6nN03ycGC"
      },
      "source": [
        "After training, we'll save the model so we can load it again later if we need to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "08NxMviGycGC"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict()}\n",
        "\n",
        "with open('trained_model.net', 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMIfAIVzycGC"
      },
      "source": [
        "## Sampling\n",
        "\n",
        "To sample from the trained model, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. By keeping doing this we'll generate a bunch of text.\n",
        "\n",
        "### Top K sampling\n",
        "\n",
        "Our predictions come from a categorical probability distribution over all the possible characters. We can make the sample text and make it more reasonable to handle (with less variables) by only considering some $K$ most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text.\n",
        "\n",
        "In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gbzwujN-ycGC"
      },
      "outputs": [],
      "source": [
        "def generate_sample(net, sample_length, prime='The ', top_k=None, cuda=False):\n",
        "    if cuda:\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "\n",
        "    net.eval()\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    \n",
        "    h = net.init_hidden(1)\n",
        "    \n",
        "    for ch in prime:\n",
        "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    for ii in range(sample_length):\n",
        "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7sL3pHUycGC",
        "outputId": "4d2b606d-b46b-4b48-cfa0-91b710ab2a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The common means that he'll be a boy.\n",
            "What went that they did beg in the son, then?\n",
            "\n",
            "BANQUO:\n",
            "What was it so sun? Who with the commonwealth this blood,\n",
            "stoop of men' shall construe the commendations,\n",
            "that I will do, the will shall serve the mother.\n",
            "\n",
            "PRINCE HENRY:\n",
            "I do speak a man, thy shepherds and hands.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Well, the poor foolish seems of treasons shall show my\n",
            "friends.\n",
            "\n",
            "Servant:\n",
            "What a thousand strength thou dost not be so monally\n",
            "the worst of this tongue?\n",
            "\n",
            "PRINCE HENRY:\n",
            "Who, he is a gentleman to make me tell me where\n",
            "is the case?\n",
            "\n",
            "First Citizen:\n",
            "Touch, so much as you'll be so much, an a money.\n",
            "\n",
            "SIR HUGH EVANS:\n",
            "I do not love, sir, well must need of my boot of a mind;\n",
            "they are not so many a thene; and when I have\n",
            "shronged is an all, and they have the cannon, to but as\n",
            "stoop that I had been a matter; I will not break a prince\n",
            "thou shalt be sad. Hush is not all to hear him this\n",
            "cause? were you so so, though I warrant you, sir?\n",
            "\n",
            "BOYET:\n",
            "That's more than I'll not speak.\n",
            "Why, that when they are better than a brother,\n",
            "Who he's no soldiers with a shame to should:\n",
            "If you'll drink send another to thy leg.\n",
            "Will you be true? why will you tell to her?\n",
            "\n",
            "LAUNCELOT:\n",
            "Will you be proud? who is't that the more impart\n",
            "the men that the seal drinks as to me to save the\n",
            "soul is all watchers in heaven or here?\n",
            "\n",
            "CASSIO:\n",
            "Hark you, madam, and tell them, and break him out of\n",
            "my legath; then he will could name the common thing,\n",
            "and that have not held some capable to service\n",
            "and made me with her stand to most heavy words.\n",
            "\n",
            "SIMONIDES:\n",
            "Here shall he serve a corse that they are blest\n",
            "That his approofs and will I'll terribly\n",
            "The country which you have done so, my lord,\n",
            "And then the worthiest common shall not stay\n",
            "To take his chambition in himself to me;\n",
            "And thou wilt shed the princess and the streets,\n",
            "That to this hour of thine ear take the soldier.\n",
            "This.\n",
            "\n",
            "CORIOLANUS:\n",
            "Troops, sir, thou wast taken her.\n",
            "\n",
            "SEBASTIAN:\n",
            "What,\n",
            "his son to that shepherd should the wars are to thrice?\n",
            "\n",
            "CORIOLANUS:\n",
            "Hav\n"
          ]
        }
      ],
      "source": [
        "sample = generate_sample(net, 2000, top_k=5, cuda=True)\n",
        "\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_text_statistics(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M55FRoL7kV5e",
        "outputId": "2c2e132c-a356-482d-f2b5-40cb7eb0784d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Flesch reading ease': 87.25,\n",
              " 'Linsear-Write Formula': 5.875,\n",
              " 'Dale-Chall readability score': 6.66}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}